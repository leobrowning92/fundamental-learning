{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#2019-04-26_week09_fundamental-learning_whatis-pytorchnn-tute\" data-toc-modified-id=\"2019-04-26_week09_fundamental-learning_whatis-pytorchnn-tute-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>2019-04-26_week09_fundamental-learning_whatis-pytorchnn-tute</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#FashionMNIST-data-import-and-check\" data-toc-modified-id=\"FashionMNIST-data-import-and-check-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>FashionMNIST data import and check</a></span></li><li><span><a href=\"#load-up-MNIST-data\" data-toc-modified-id=\"load-up-MNIST-data-1.0.3\"><span class=\"toc-item-num\">1.0.3&nbsp;&nbsp;</span>load up MNIST data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Aside-on-using-lists-of-indices-to-slice-np-arrays\" data-toc-modified-id=\"Aside-on-using-lists-of-indices-to-slice-np-arrays-1.0.3.1\"><span class=\"toc-item-num\">1.0.3.1&nbsp;&nbsp;</span>Aside on using lists of indices to slice np arrays</a></span></li></ul></li></ul></li><li><span><a href=\"#Neural-netw-from-scratch-(no-torch.nn)\" data-toc-modified-id=\"Neural-netw-from-scratch-(no-torch.nn)-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Neural netw from scratch (no torch.nn)</a></span><ul class=\"toc-item\"><li><span><a href=\"#run-through-a-training-loop\" data-toc-modified-id=\"run-through-a-training-loop-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>run through a training loop</a></span></li></ul></li><li><span><a href=\"#refactor-to-use-nn-specific-code\" data-toc-modified-id=\"refactor-to-use-nn-specific-code-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>refactor to use nn specific code</a></span><ul class=\"toc-item\"><li><span><a href=\"#using-nn.sequential-to-replace-a-custom-model-class\" data-toc-modified-id=\"using-nn.sequential-to-replace-a-custom-model-class-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>using nn.sequential to replace a custom model class</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# 2019-04-26_week09_fundamental-learning_whatis-pytorchnn-tute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn \n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FashionMNIST data import and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "fmnist_train = torchvision.datasets.FashionMNIST(\"data/Fashion-MNIST/\", \n",
    "                                                download = True,\n",
    "                                                train=True,\n",
    "                                                transform =transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(fmnist_train,\n",
    "                                          batch_size=64,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=4)\n",
    "\n",
    "fmnist_test = torchvision.datasets.FashionMNIST(\"data/Fashion-MNIST/\", \n",
    "                                                download = True,\n",
    "                                                train=False,\n",
    "                                                transform=transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(fmnist_test,\n",
    "                                          batch_size=64,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=4)\n",
    "\n",
    "category_labels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \n",
    "          \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(category_labels[labels[0].item()])\n",
    "plt.imshow(images[0,0,:,:].numpy(),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load up MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is important as some of the initial matrix math seems only to work with the vectors used, and I dont want to overcomplicate things inititally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "#was n and c in the example code\n",
    "train_size, image_size = x_train.shape\n",
    "x_train, x_train.shape, y_train.min(), y_train.max()\n",
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside on using lists of indices to slice np arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "passing a list of indices to a numpy array returns those indeces. eg\n",
    "\n",
    "`x[[0,1,2],[4,3,1]]`\n",
    "returns`[x[0,4], x[1,3], x[2,1]]`\n",
    "\n",
    "\n",
    "and is equivelent to `x[range(3),[4,3,1]]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural netw from scratch (no torch.nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "x_batch = x_train[:batch_size]\n",
    "predictions = model(x_batch)\n",
    "print(predictions[0], predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note can use no log, as the softmax layer has the log in it.\n",
    "# this function is actually just the \n",
    "def negative_log_likelihood(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "loss_func = negative_log_likelihood\n",
    "\n",
    "def accuracy(out,yb):\n",
    "    preds = torch.argmax(out,dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_batch = y_train[:batch_size]\n",
    "print(f\"baseline loss : {loss_func(predictions,y_batch).item():0.3f}\") \n",
    "print(f\"baseline accuracy : {accuracy(predictions,y_batch).item()*100:0.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run through a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 0.5\n",
    "epochs =2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((train_size-1)//batch_size+1):\n",
    "        start_i = i*batch_size\n",
    "        end_i = start_i + batch_size\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred,yb)\n",
    "        if i%100==0:\n",
    "            print(f\"{epoch:03d}, {i:05d}  loss : {loss:0.3f}\")\n",
    "        loss.backward()\n",
    "        # since we are doing things manually\n",
    "        # we dont want these operations added to the calculations\n",
    "        # for the next calculation of the gradient\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * learning_rate\n",
    "            bias -= bias.grad * learning_rate\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"loss : {loss_func(model(x_batch),y_batch).item():0.3f}\") \n",
    "print(f\"accuracy : {accuracy(model(x_batch),y_batch).item()*100:0.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## refactor to use nn specific code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784,10)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)\n",
    "\n",
    "class Mnist_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size = 3, stride = 2, padding =1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size = 3, stride = 2, padding =1)\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size = 3, stride = 2, padding =1)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1,1,28,28)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "        xb = F.avg_pool2d(xb, 4)\n",
    "        return xb.view(-1, xb.size(1))\n",
    "        \n",
    "\n",
    "\n",
    "def fit(model, optimizer, train_dl, valid_dl, loss_func,\n",
    "        epochs=2, v=True):\n",
    "    \n",
    "    model.train() #used  by dropout and batchnorm2d for different behaviour\n",
    "    for epoch in range(epochs):\n",
    "        for i, data in enumerate(train_dl):\n",
    "            xb, yb = data\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred,yb)\n",
    "            if i%100==0 and v:\n",
    "                print(f\"{epoch:03d}, {i:05d}  loss : {loss:0.3f}\")\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        model.eval() #used  by dropout and batchnorm2d for different behaviour\n",
    "        with torch.no_grad():\n",
    "            valid_loss = sum(loss_func(model(xb),yb) for xb, yb in valid_dl)\n",
    "        print(f\"epoch {epoch:03d}, validation loss : {valid_loss / len(valid_dl):0.3f}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_valid_ds = TensorDataset(x_valid, y_valid)\n",
    "mnist_train_ds = TensorDataset(x_train,y_train)\n",
    "\n",
    "def get_data(train_ds, valid_ds,batch_size):\n",
    "    train_dl = DataLoader(train_ds, batch_size = batch_size, shuffle = True)\n",
    "    # validation takes less memory as no backprop is used, \n",
    "    # hence a bigger batch size can be used\n",
    "    valid_dl = DataLoader(valid_ds, batch_size*2) \n",
    "    \n",
    "    return train_dl, valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl = get_data(mnist_train_ds, mnist_valid_ds, batch_size=64)\n",
    "linear_model = Mnist_Logistic()\n",
    "optimizer = optim.SGD(linear_model.parameters(), lr=0.5)\n",
    "fit(linear_model, optimizer, train_dl, valid_dl, loss_func,\n",
    "    epochs=3, v=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnn_model = Mnist_CNN()\n",
    "optimizer = optim.SGD(cnn_model.parameters(), lr=0.1, momentum = 0.9)\n",
    "fit(cnn_model, optimizer, train_dl, valid_dl, loss_func,\n",
    "    epochs=3, v=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using nn.sequential to replace a custom model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sequential doesnt have a `view` layer, so define a custom layer:\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "    \n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)\n",
    "\n",
    "\n",
    "seq_cnn_model = nn.Sequential(\n",
    "    Lambda(preprocess),\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4), # adaptive defines the size of the output tensor so flexi\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(seq_cnn_model.parameters(), lr=0.1, momentum = 0.9)\n",
    "fit(seq_cnn_model, optimizer, train_dl, valid_dl, loss_func,\n",
    "    epochs=3, v=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
