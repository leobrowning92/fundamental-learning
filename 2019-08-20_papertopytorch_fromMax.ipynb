{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper to pytorch examples from Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highwaynet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1505.00387"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of a Highway block is that each layer adds some nonlinear transform (eg. sigmoid) transform T \n",
    "which determines the proportion of the layer that is passed through the affine transformation H, \n",
    "and conversely C (cary) = 1-T which determines a straight passthrough of input to output. \n",
    "Note that for this to be valid the input and output of the highway block must have the same dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class HighwayBlock(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(HighwayBlock, self).__init__()\n",
    "        self.H = nn.Linear(input_dim, input_dim)\n",
    "        self.T = nn.Linear(input_dim, input_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Negative intialization of bias (to -1)\n",
    "        # So that the layer is initially biassed towards carry behaviour\n",
    "        self.T.bias.data *= 0.\n",
    "        self.T.bias.data -= 1.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # usually an affine transformation\n",
    "        h = self.H(x)\n",
    "        # transform gate\n",
    "        t = self.sigmoid(self.T(x))\n",
    "        # carry gate\n",
    "        c = 1-t\n",
    "        return h * t + c * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highway = HighwayBlock(1024)\n",
    "summary(highway,(1,1024))\n",
    "highway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ULMFiT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1801.06146"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ULMFiT(nn.Module):\n",
    "    def __init__(self, hidden_dim=1150, embedding_dim=400, vocab_size=100000, rnn_layers=3, weight_drop=0.5):\n",
    "        super(ULMFiT, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers=rnn_layers, batch_first=True, bidirectional=True)\n",
    "        self.lm_head = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        self.rnn_layers = rnn_layers\n",
    "        self.weight_drop = weight_drop\n",
    "        self.weight_cache = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.drop_connect()\n",
    "        h, _ = self.rnn(x)\n",
    "        self.restore_weights()\n",
    "        return h\n",
    "\n",
    "    def drop_connect(self):\n",
    "        \"\"\"Randomly sets H-H weights to zero and caches weights.\"\"\"\n",
    "        self.weight_cache = {}\n",
    "        for i in range(self.rnn_layers):\n",
    "            for name, param in self.rnn.named_parameters():\n",
    "                if f'weight_hh_l{i}' in name:\n",
    "                    shape = param.data.shape\n",
    "                    mask = torch.rand(param.data.view(-1).shape[0]) > self.weight_drop\n",
    "                    mask = mask.view(shape)\n",
    "                    inv_mask = 1 - mask\n",
    "                    self.weight_cache[name] = param.data * inv_mask.float()\n",
    "                    param.data = param.data * mask.float()\n",
    "\n",
    "    def restore_weights(self):\n",
    "        for i in range(self.rnn_layers):\n",
    "            for name, param in self.rnn.named_parameters():\n",
    "                if f'weight_hh_l{i}' in name:\n",
    "                    param.data += self.weight_cache[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulmfit=ULMFiT()\n",
    "ulmfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1409.1556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, n_conv=16, im_size=224):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1, stride=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1, stride=1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1, stride=1)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1)\n",
    "        self.conv7 = nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1)\n",
    "        self.conv8 = nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1)\n",
    "\n",
    "        self.conv9 = nn.Conv2d(256, 512, kernel_size=3, padding=1, stride=1)\n",
    "        self.conv10 = nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1)\n",
    "        self.conv11 = nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1)\n",
    "        self.conv12 = nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1)\n",
    "\n",
    "        self.conv13 = nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1)\n",
    "        self.conv14 = nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1)\n",
    "        self.conv15 = nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1)\n",
    "        self.conv16 = nn.Conv2d(512, 512, kernel_size=3, padding=1, stride=1)\n",
    "\n",
    "        self.im_flatten = (im_size // 32) ** 2 * 512\n",
    "        self.fc1 = nn.Linear(self.im_flatten, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.out = nn.Linear(4096, 1000)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "    # Conv stack\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.conv9(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv10(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv11(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv12(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv13(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv14(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv15(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv16(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(-1, self.im_flatten)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.out(x)\n",
    "        y = self.softmax(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My rewrite using the `nn.Sequential` module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referenced [this tutorial](https://github.com/FrancescoSaverioZuppichini/Pytorch-how-and-when-to-use-Module-Sequential-ModuleList-and-ModuleDict) to rewrite the network using `nn.Sequential` to make more modular and readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_f,out_f,**kwargs):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_f, out_f, **kwargs),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "def conv_stack_block(in_f,out_f,stack_size,**kwargs):\n",
    "    upstack = conv_block(in_f, out_f,**kwargs)\n",
    "    stack_body = [conv_block(out_f,out_f,**kwargs) for _ in range(stack_size-1)]\n",
    "    stack_block = nn.Sequential(\n",
    "        upstack,\n",
    "        *stack_body,\n",
    "        nn.MaxPool2d(2, stride=2)\n",
    "    )\n",
    "    return stack_block\n",
    "    \n",
    "\n",
    "class SequentialVGG16(nn.Module):\n",
    "    def __init__(self, n_conv=16, im_size=224):\n",
    "        super(SequentialVGG16, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(OrderedDict([\n",
    "            ('convblock1',conv_stack_block(3, 64, stack_size = 2, kernel_size=3, padding=1, stride=1 )),\n",
    "            ('convblock2',conv_stack_block(64, 128, stack_size = 2, kernel_size=3, padding=1, stride=1 )),\n",
    "            ('convblock3',conv_stack_block(128, 256, stack_size = 4, kernel_size=3, padding=1, stride=1 )),\n",
    "            ('convblock4',conv_stack_block(256, 512, stack_size = 4, kernel_size=3, padding=1, stride=1 )),\n",
    "            ('convblock5',conv_stack_block(512, 512, stack_size = 4, kernel_size=3, padding=1, stride=1 )),\n",
    "        ]))\n",
    "        \n",
    "        self.im_flatten = (im_size // 32) ** 2 * 512\n",
    "        \n",
    "        self.fc_output = nn.Sequential(\n",
    "            nn.Linear(self.im_flatten, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 1000),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(-1, self.im_flatten)\n",
    "        y = self.fc_output(x)\n",
    "        return y\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_max=VGG16()\n",
    "vgg_seq=SequentialVGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(vgg_seq, input_size=(3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1609.03499"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConvBlock(nn.Module):\n",
    "    def __init__(self, channels=32, kernel=3):\n",
    "        super(CausalConvBlock, self).__init__()\n",
    "        self.kernel_size = kernel\n",
    "        self.gate_cnn = nn.Conv1d(channels, channels, kernel_size=kernel)\n",
    "        self.filter_cnn = nn.Conv1d(channels, channels, kernel_size=kernel)\n",
    "        self.final_cnn = nn.Conv1d(channels, channels, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = self.gate_cnn(x)\n",
    "        filt = self.filter_cnn(x)\n",
    "        # Shift the conv outputs rightward (pad left side)\n",
    "        gate = torch.cat([torch.zeros_like(gate)[:, :, :(self.kernel_size - 1)], gate], dim=-1)\n",
    "        filt = torch.cat([torch.zeros_like(filt)[:, :, :(self.kernel_size - 1)], filt], dim=-1)\n",
    "\n",
    "        z = self.tanh(filt) * self.sigmoid(gate)\n",
    "        z = self.final_cnn(z)\n",
    "        # Add residual connection\n",
    "        return z + x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer (encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.weight * x + self.bias\n",
    "\n",
    "\n",
    "class ScaledDotAttention(nn.Module):\n",
    "    def __init__(self, scale, drop_p=0.1):\n",
    "        super(ScaledDotAttention, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "\n",
    "    def forward(self, q, k , v):\n",
    "        attn = torch.matmul(q, k.transpose(-1, -2)) * self.scale # Should be (BATCH, T, T)\n",
    "        attn = self.softmax(attn)\n",
    "        attn = self.dropout(attn)\n",
    "        return torch.matmul(attn, v)  # Should result in (BATCH, T, d_attn)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, scale, dmodel, num_heads=8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.v_proj = [nn.Linear(dmodel, int(dmodel / num_heads), bias=False).to(DEVICE) for _ in range(num_heads)]\n",
    "        self.k_proj = [nn.Linear(dmodel, int(dmodel / num_heads), bias=False).to(DEVICE) for _ in range(num_heads)]\n",
    "        self.q_proj = [nn.Linear(dmodel, int(dmodel / num_heads), bias=False).to(DEVICE) for _ in range(num_heads)]\n",
    "        self.scaled_attention = ScaledDotAttention(scale)\n",
    "\n",
    "        self.out = nn.Linear(dmodel, dmodel, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attns = []\n",
    "        for v_proj, k_proj, q_proj in zip(self.v_proj, self.k_proj, self.q_proj):\n",
    "            temp_v = v_proj(x)\n",
    "            temp_k = k_proj(x)\n",
    "            temp_q = q_proj(x)\n",
    "            attns.append(self.scaled_attention(temp_v, temp_k, temp_q))\n",
    "\n",
    "        attention = torch.cat(attns, dim=-1)\n",
    "        return self.out(attention)\n",
    "\n",
    "\n",
    "class FFBlock(nn.Module):\n",
    "    def __init__(self, dmodel, dff=2048):\n",
    "        super(FFBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(dmodel, dff)\n",
    "        self.fc2 = nn.Linear(dff, dmodel)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, scale, dmodel=512, num_heads=8):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mh_attention = MultiHeadAttention(scale=scale, dmodel=dmodel, num_heads=num_heads)\n",
    "        self.ffblock = FFBlock(dmodel)\n",
    "        self.layer_norm = LayerNorm(dmodel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn = self.mh_attention(x)\n",
    "        x = self.layer_norm(x + attn) # \"add and norm\"\n",
    "        ff = self.ffblock(x)\n",
    "        x = self.layer_norm(x + ff)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_encoders=6, dmodel=512, num_heads=8):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.encoders = [EncoderLayer(dmodel, num_heads=num_heads).to(DEVICE) for _ in range(num_encoders)]\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # Run embeddings through the encoders\n",
    "        enc_outs = []\n",
    "        for encoder in self.encoders:\n",
    "            enc_outs.append(encoder(embeddings))\n",
    "        # Sum all of the stacked transformer encodings\n",
    "        return torch.sum(torch.stack(enc_outs), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
