{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019-10-22_fundamentallearning_pytorchWordRNN_translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- follow this tutorial https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "- use reloading on training loop https://github.com/julvo/reloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import requests\n",
    "import unicodedata\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get french english translations from https://www.manythings.org/anki/fra-eng.zip and put them in a folder `data/pytorch_tutorial/fra-eng/fra.txt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start and end tokens\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    \"\"\"Helper class to manage index <=> word translation\"\"\"\n",
    "    def __init__(self,name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.index2word = {SOS_token:'SOS', EOS_token:'EOS'}\n",
    "        self.word2count = {} # for rare words\n",
    "        self.n_words = 2\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.word2count[word] = 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD',s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    \n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    # adds a space before end of sentance punctuation\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s) \n",
    "    #replaces all other punctuation or unusual characters with spaces\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)     \n",
    "    return s\n",
    "    \n",
    "def sentence2indexlist(lang, s):\n",
    "    return [lang.word2index[w] for w in s.split(' ')]\n",
    "\n",
    "def sentence2tensor(lang,s):\n",
    "    indexes = sentence2indexlist(lang, s)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes ,dtype=torch.long).view(-1,1)\n",
    "\n",
    "def pair2tensors(l1,l2,pair):\n",
    "    input_tensor = sentence2tensor(l1,pair[0])\n",
    "    target_tensor = sentence2tensor(l2,pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter for short sentences that start with 'i am', 'he is' etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ENG_LENGTH = 10\n",
    "ENG_PREFIXES = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def check_pair(p):\n",
    "    return len(p[0].split(' ')) < MAX_ENG_LENGTH and p[0].startswith(ENG_PREFIXES)\n",
    "def filter_pairs(pairs):\n",
    "    # file is english french, so must reverse when filtering\n",
    "    return [p[::-1] for p in pairs if check_pair(p)]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(lang1, lang2):\n",
    "    print('Loading data')\n",
    "    with open(f'data/pytorch_tutorial/{lang1}-{lang2}/fra.txt') as f:\n",
    "        pairs = [[ normalizeString(s) for s in line.split('\\t')[:2]] for line in f]\n",
    "    print('filtering data')\n",
    "    pairs = filter_pairs(pairs)\n",
    "    print(random.choice(pairs))\n",
    "    l1 = Lang(lang1)\n",
    "    l2 = Lang(lang2)\n",
    "    \n",
    "    for p in pairs:\n",
    "        l1.add_sentence(p[0])\n",
    "        l2.add_sentence(p[1])\n",
    "    print(f'{l1.name} : {l1.n_words}')\n",
    "    print(f'{l2.name} : {l2.n_words}')\n",
    "    return l1, l2, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "filtering data\n",
      "['si j ai offusque quelqu un je m en excuse .', 'i m sorry if i offended anyone .']\n",
      "fra : 4982\n",
      "eng : 3231\n"
     ]
    }
   ],
   "source": [
    "fra_lang, eng_lang, sentence_pairs = prepare_data('fra', 'eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['j ai ans .', 'i m .']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[2],\n",
       "         [3],\n",
       "         [4],\n",
       "         [5],\n",
       "         [1]]), tensor([[2],\n",
       "         [3],\n",
       "         [4],\n",
       "         [1]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sentence_pairs[0])\n",
    "pair2tensors(fra_lang, eng_lang, sentence_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2],\n",
       "        [  3],\n",
       "        [ 49],\n",
       "        [786],\n",
       "        [  1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"I'm a cat\"\n",
    "sentence2tensor(eng_lang,normalizeString(test))\n",
    "# sentence2indexlist(eng_lang,normalizeString(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, inout_dim, hidden_dim=128):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(inout_dim, hidden_dim)\n",
    "        self.encoder = nn.GRU(hidden_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        embedding = self.embedding(x).view(1,1,-1)\n",
    "        output, hidden = self.encoder(embedding,hidden)\n",
    "        return output,hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_dim)\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, inout_dim, hidden_dim=128):\n",
    "        super(DecoderRNN,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(inout_dim, hidden_dim)\n",
    "        self.encoder = nn.GRU(hidden_dim, hidden_dim)\n",
    "        self.hidden2out = nn.Linear(hidden_dim, inout_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim =1)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        embedding = self.embedding(x).view(1,1,-1)\n",
    "        output, hidden = self.encoder(embedding, hidden)\n",
    "        output = self.softmax(self.hidden2out(output[0]))\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_dim)\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = EncoderRNN(fra_lang.n_words)\n",
    "dec = DecoderRNN(eng_lang.n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy pass into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_pair = random.choice(sentence_pairs)\n",
    "dummy_in = sentence2tensor(fra_lang, dummy_pair[0])\n",
    "dummy_out = sentence2tensor(eng_lang, dummy_pair[1])\n",
    "enc_h0 = enc.init_hidden()\n",
    "h = enc_h0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nous sommes realistes .', 'we re realistic .']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(dummy_in.size(0)):\n",
    "    _, h = enc(dummy_in[i],h)\n",
    "enc_h = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = []\n",
    "hidden = enc_h\n",
    "output = torch.tensor([[SOS_token]])\n",
    "for i in range(dummy_out.size(0)):\n",
    "    output, h = dec(output,hidden)\n",
    "\n",
    "    top_value, top_index = output.topk(1)  \n",
    "    output = top_index.squeeze().detach()\n",
    "    outs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(2141), tensor(2097), tensor(1097), tensor(732), tensor(1238)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(input_tensor, target_tensor, enc, dec, optimizer, criterion):\n",
    "    encoder_hidden = enc.init_hidden()\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    target_length = input_tensor.size(0)\n",
    "    \n",
    "    for ei in range(target_length):\n",
    "        _, encoder_hidden = enc(input_tensor[ei], encoder_hidden)\n",
    "        \n",
    "    decoder_input = torch.tensor([[SOS_token]])\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    for di in range(target_tensor.size(0)):\n",
    "        decoder_input, decoder_hidden = dec(decoder_input, decoder_hidden)\n",
    "        \n",
    "        # for nll output = [batchsize, number of classes (words)] and target = correct class\n",
    "        loss += criterion(decoder_input,target_tensor[di])\n",
    "        decoder_input = target_tensor[di] #teacher forcing\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()/ target_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(itertools.chain(enc.parameters(),dec.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, n_iters, print_every):\n",
    "    training_set = [pair2tensors(fra_lang,eng_lang, random.choice(sentence_pairs)) for i in range(n_iters)]\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for i, pair in enumerate(training_set):\n",
    "            input_tensor = pair[0]\n",
    "            target_tensor = pair[1]\n",
    "            loss_total =0\n",
    "            loss_total += train_step(input_tensor, target_tensor, enc, dec, optimizer, criterion)\n",
    "\n",
    "        if e%print_every ==0:\n",
    "            print (f'epoch{e:3} loss:{loss_total:5.2f}')\n",
    "            loss_total = 0\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 loss: 2.75\n",
      "epoch  1 loss: 1.85\n",
      "epoch  2 loss: 1.43\n",
      "epoch  3 loss: 1.11\n",
      "epoch  4 loss: 0.88\n",
      "epoch  5 loss: 0.73\n",
      "epoch  6 loss: 0.63\n",
      "epoch  7 loss: 0.56\n",
      "epoch  8 loss: 0.50\n",
      "epoch  9 loss: 0.45\n"
     ]
    }
   ],
   "source": [
    "train(10, 100,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
