{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#2019-05-17_week12_fundamental-learning_openAIgym-deepQ-exploration\" data-toc-modified-id=\"2019-05-17_week12_fundamental-learning_openAIgym-deepQ-exploration-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>2019-05-17_week12_fundamental-learning_openAIgym-deepQ-exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#model-definition\" data-toc-modified-id=\"model-definition-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>model definition</a></span></li><li><span><a href=\"#lr-exploration\" data-toc-modified-id=\"lr-exploration-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>lr exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#memory-wrt-learning-rate-hyperparameter-matrix\" data-toc-modified-id=\"memory-wrt-learning-rate-hyperparameter-matrix-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>memory wrt learning rate hyperparameter matrix</a></span></li></ul></li><li><span><a href=\"#Eps-greedy-exploration\" data-toc-modified-id=\"Eps-greedy-exploration-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Eps greedy exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Form-of-the-decaying-epsilon-greedy-algorithm\" data-toc-modified-id=\"Form-of-the-decaying-epsilon-greedy-algorithm-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Form of the decaying epsilon greedy algorithm</a></span></li></ul></li><li><span><a href=\"#Review-of-first-attempt-at-Reinforcement-learning\" data-toc-modified-id=\"Review-of-first-attempt-at-Reinforcement-learning-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Review of first attempt at Reinforcement learning</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Done-well\" data-toc-modified-id=\"Done-well-1.4.0.1\"><span class=\"toc-item-num\">1.4.0.1&nbsp;&nbsp;</span>Done well</a></span></li><li><span><a href=\"#Points-of-difficulty\" data-toc-modified-id=\"Points-of-difficulty-1.4.0.2\"><span class=\"toc-item-num\">1.4.0.2&nbsp;&nbsp;</span>Points of difficulty</a></span></li><li><span><a href=\"#Thinking-points-for-next-project\" data-toc-modified-id=\"Thinking-points-for-next-project-1.4.0.3\"><span class=\"toc-item-num\">1.4.0.3&nbsp;&nbsp;</span>Thinking points for next project</a></span></li></ul></li></ul></li><li><span><a href=\"#Rendering-for-presentation\" data-toc-modified-id=\"Rendering-for-presentation-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Rendering for presentation</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019-05-17_week12_fundamental-learning_openAIgym-deepQ-exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import gym\n",
    "import attr\n",
    "import random\n",
    "import math\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pdb\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\"Experience\", (\"state\", \"action\", \"reward\", \"next_state\"))\n",
    "\n",
    "\n",
    "@attr.s\n",
    "class ReplayMemory(object):\n",
    "    capacity = attr.ib()\n",
    "    memory = []\n",
    "    position = 0\n",
    "\n",
    "    def push(self, transition):\n",
    "        \"\"\"adds experiences to the memory buffer\"\"\"\n",
    "        self.memory.append(transition)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class CPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(4, 200)\n",
    "        # self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xb = x.view(-1, 4)\n",
    "        xb = F.relu(self.fc1(xb))\n",
    "        # xb = F.relu(self.fc2(xb))\n",
    "        xb = self.fc3(xb)\n",
    "        return xb.view(-1, xb.size(1))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class CPSolver(object):\n",
    "    \"\"\" \n",
    "    This is the base class that manages the whole RL pipeline.\n",
    "    This was written for my own understanding, but it runs and \n",
    "    hopefully provides a bit of insight.\n",
    "\n",
    "    Args:\n",
    "        episodes (int): Number of runs of the game (up to end) to perform\n",
    "        memory (int): Number of memories in the memory buffer\n",
    "        gamma (float): Parameter in the loss function determining the \n",
    "            fractional impact of actions. between 0 and 1\n",
    "        lr (float): The learning rate of the gradient descent when training the network\n",
    "        batch_size (int): Number of memories passed in for training simulataneously\n",
    "        eps_start (float): Starting fraction of true random decisions\n",
    "        eps_end (float): Final fraction of true random decisions\n",
    "        eps_decay (int): Exponential decay constant for the proportion of\n",
    "            random decisions in episodes. See self.eps_greedy for \n",
    "            implementation of decaying epsilon greedy strategy\n",
    "        optimizer (torch optimizer): SGD = stochastic gradient descent. could use others\n",
    "        loss_fn (torch loss): MSE = Mean square error loss\n",
    "        render (bool): Whether you want to see the game run\n",
    "        render_step (int): render the game every x steps\n",
    "        output = False: For saving video files. requires ffmpeg\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        episodes=300,\n",
    "        memory=10000,\n",
    "        gamma=0.8,\n",
    "        lr=0.01,\n",
    "        batch_size=32,\n",
    "        eps_start=0.9,\n",
    "        eps_end=0.01,\n",
    "        eps_decay=100,\n",
    "        optimizer=optim.SGD,\n",
    "        loss_fn=nn.MSELoss,\n",
    "        render = True,\n",
    "        render_step = 100,\n",
    "        output = False\n",
    "    ):\n",
    "\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_decay = eps_decay\n",
    "        self.episodes = episodes\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.memory_size = memory\n",
    "        \n",
    "        self.memory = ReplayMemory(memory)\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        self.model = CPNet()\n",
    "        self.optimizer = optimizer(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn=loss_fn()\n",
    "        \n",
    "        self.render = render\n",
    "        self.render_step = render_step\n",
    "        if output:\n",
    "            # enables video file output\n",
    "            self.env = gym.wrappers.Monitor(self.env, f'./RL_vids/{str(time())}/',video_callable=self.render_check)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def render_check(self, step):\n",
    "        if step==0:\n",
    "            return True\n",
    "        else:\n",
    "            return (step+1)%self.render_step==0\n",
    "    \n",
    "    def eps_threshold(self, steps_done):\n",
    "        return self.eps_end + (self.eps_start - self.eps_end) * math.exp(\n",
    "            -1.0 * steps_done / self.eps_decay\n",
    "        )\n",
    "\n",
    "    def select_action(self, state, steps_done):\n",
    "        \"\"\"Selects the best action using the model\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # model predicts highest predicted reward\n",
    "            prediction = self.model(state)\n",
    "            # selects the action with the highest predicted probability\n",
    "            action = prediction.data.max(1)[1].view(1, 1)\n",
    "        return action\n",
    "            \n",
    "        \n",
    "    def eps_greedy(self, state, steps_done):\n",
    "        if random.random() > self.eps_threshold(steps_done):\n",
    "            return self.select_action(state, steps_done)\n",
    "        else:\n",
    "            return torch.tensor([[random.choice([0, 1])]])\n",
    "        \n",
    "\n",
    "    def optimize_model(self):\n",
    "        \n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        # print(transitions)\n",
    "\n",
    "        batch_state, batch_action, batch_reward, batch_next_state = zip(*transitions)\n",
    "\n",
    "        batch_state = torch.cat(batch_state)\n",
    "        batch_action = torch.cat(batch_action)\n",
    "        batch_reward = torch.cat(batch_reward)\n",
    "        batch_next_state = torch.cat(batch_next_state)\n",
    "        # print(f'batch_reward:{batch_reward}')\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        # The network returns probabilities\n",
    "        # The probs corresponding to the actions taken are selected\n",
    "        current_q_values = self.model(batch_state).gather(1, batch_action).view(-1)\n",
    "        # best probabilities possible from the next state\n",
    "        max_next_q_values = self.model(batch_next_state).detach().max(1)[0]\n",
    "        expected_q_values = batch_reward + (self.gamma * max_next_q_values)\n",
    "        # print(batch_state.shape, batch_action.shape, batch_reward.shape, batch_next_state.shape)\n",
    "        # print(current_q_values.shape, expected_q_values.shape)\n",
    "        # print(current_q_values, expected_q_values)\n",
    "        \n",
    "        # loss is measured from error between current and newly expected Q values\n",
    "        loss = self.loss_fn(current_q_values, expected_q_values)\n",
    "        # backpropagation of loss to NN\n",
    "\n",
    "        loss.backward()\n",
    "        # print(f'ep:{episode:03d}-step:{i:03d}-loss:{loss:0.4f}')\n",
    "        # print(current_q_values.mean().item(), expected_q_values.mean().item())\n",
    "        # print(optimizer.param_groups[0]['params'][0].grad.mean().item(),\n",
    "        #       optimizer.param_groups[0]['params'][0].grad.var().item())\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def learn(self):\n",
    "        steps_done = 0\n",
    "        ep_count = []\n",
    "        step_count = []\n",
    "        \n",
    "        for episode in range(self.episodes):\n",
    "            state = self.env.reset()\n",
    "            for i in count():\n",
    "                if self.render and (episode+1) % self.render_step ==0:\n",
    "                    if i==0:\n",
    "                        print(f'lr:{self.lr}_mem:{self.memory_size}_Episode:{episode+1}')\n",
    "                    self.env.render()\n",
    "                action = self.eps_greedy(torch.FloatTensor([state]), steps_done)\n",
    "                steps_done += 1\n",
    "                next_state, reward, done, info = self.env.step(action[0, 0].item())\n",
    "                if done:\n",
    "                    reward = -1\n",
    "                self.memory.push(\n",
    "                    (\n",
    "                        torch.FloatTensor(state),\n",
    "                        action,\n",
    "                        torch.FloatTensor([reward]),\n",
    "                        torch.FloatTensor(next_state),\n",
    "                    )\n",
    "                )\n",
    "                # Only train if there are enough memories to produce a full batch\n",
    "                if len(self.memory) >= self.batch_size:\n",
    "                    self.optimize_model()\n",
    "                    \n",
    "                state = next_state\n",
    "                if done:\n",
    "                    ep_count.append(episode)\n",
    "                    step_count.append(i)\n",
    "                    break\n",
    "        return ep_count, step_count\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "cartpole_solver = CPSolver()\n",
    "ep_count, steps = cartpole_solver.learn()\n",
    "cartpole_solver.close()\n",
    "ax.plot(steps, label=f\"lr={lr} mem={mem}\")\n",
    "plt.legend()\n",
    "plt.title(f\"SGD_MSEloss_lr-{lr_space}_mem-{mem_space}_{eps}eps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## lr exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "lr_space = [ 0.001, 0.01, 0.1]\n",
    "mem_space = [1000]\n",
    "eps = 100\n",
    "\n",
    "for lr in lr_space:\n",
    "    for mem in mem_space:\n",
    "        cartpole_solver = CPSolver(\n",
    "            episodes=eps,\n",
    "            memory=mem,\n",
    "            gamma=0.8,\n",
    "            lr=lr,\n",
    "            batch_size=32,\n",
    "            eps_start=0.9,\n",
    "            eps_end=0.05,\n",
    "            eps_decay=200,\n",
    "            output=True,\n",
    "            render=True,\n",
    "            render_step=eps,\n",
    "            optimizer=optim.SGD,\n",
    "            loss_fn = nn.MSELoss\n",
    "        )\n",
    "        ep_count, steps = cartpole_solver.learn()\n",
    "        cartpole_solver.close()\n",
    "        ax.plot(steps, label=f\"lr={lr} mem={mem}\")\n",
    "plt.legend()\n",
    "plt.title(f\"SGD_MSEloss_lr-{lr_space}_mem-{mem_space}_{eps}eps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "params=list(model.parameters())\n",
    "for i in params:\n",
    "    print(i.shape)\n",
    "params[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nmodel=CPNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(nmodel.fc3.bias)\n",
    "nmodel.fc3.bias=params[3]\n",
    "print(nmodel.fc3.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'test_model.torch')\n",
    "\n",
    "new_model = CPNet()\n",
    "new_model.load_state_dict(torch.load('test_model.torch'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Use the save and load functionality to use a model later for inferencee."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "0.01 seems the sweet spot between learning rate and stability.\n",
    "\n",
    "It would seem that instability could be due to the sensitivity to what is coming in on each batch. if the memory size is low, then over time the early examples of performance will be replaced, and they will stop cropping up in batches. Thus the learning of the network will no longer emphasize them, and it could forget.\n",
    "\n",
    "try with longer memory time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "lr_space = [ 0.001, 0.01, 0.1]\n",
    "mem_space = [10000]\n",
    "eps = 100\n",
    "for lr in lr_space:\n",
    "    for mem in mem_space:\n",
    "        cartpole_solver = CPSolver(\n",
    "            episodes=eps,\n",
    "            memory=mem,\n",
    "            gamma=0.8,\n",
    "            lr=lr,\n",
    "            batch_size=32,\n",
    "            eps_start=0.9,\n",
    "            eps_end=0.05,\n",
    "            eps_decay=200,\n",
    "            output=False,\n",
    "            render=True,\n",
    "            render_step=eps,\n",
    "            optimizer=optim.SGD,\n",
    "            loss_fn = nn.MSELoss\n",
    "        )\n",
    "        ep_count, steps = cartpole_solver.learn()\n",
    "        cartpole_solver.close()\n",
    "        ax.plot(steps, label=f\"lr={lr} mem={mem}\")\n",
    "plt.legend()\n",
    "plt.title(f\"SGD_MSEloss_lr-{lr_space}_mem-{mem_space}_{eps}eps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Higher memory shows less drastic reduction in performance, although still high variability over a short episode scale.\n",
    "\n",
    "can try more episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "lr_space = [ 0.001, 0.01, 0.1]\n",
    "mem_space = [10000]\n",
    "eps = 300\n",
    "for lr in lr_space:\n",
    "    for mem in mem_space:\n",
    "        cartpole_solver = CPSolver(\n",
    "            episodes=eps,\n",
    "            memory=mem,\n",
    "            gamma=0.8,\n",
    "            lr=lr,\n",
    "            batch_size=32,\n",
    "            eps_start=0.9,\n",
    "            eps_end=0.05,\n",
    "            eps_decay=200,\n",
    "            output=False,\n",
    "            render=True,\n",
    "            render_step=100,\n",
    "            optimizer=optim.SGD,\n",
    "            loss_fn = nn.MSELoss\n",
    "        )\n",
    "        ep_count, steps = cartpole_solver.learn()\n",
    "        cartpole_solver.close()\n",
    "        ax.plot(steps, label=f\"lr={lr} mem={mem}\")\n",
    "plt.legend()\n",
    "plt.title(f\"SGD_MSEloss_lr-{lr_space}_mem-{mem_space}_{eps}eps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### memory wrt learning rate hyperparameter matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "lr_space = [ 0.01]\n",
    "mem_space = [1000,10000, 100000]\n",
    "eps = 300\n",
    "for lr in lr_space:\n",
    "    for mem in mem_space:\n",
    "        cartpole_solver = CPSolver(\n",
    "            episodes=eps,\n",
    "            memory=mem,\n",
    "            gamma=0.8,\n",
    "            lr=lr,\n",
    "            batch_size=32,\n",
    "            eps_start=0.9,\n",
    "            eps_end=0.05,\n",
    "            eps_decay=200,\n",
    "            output=False,\n",
    "            render=True,\n",
    "            render_step=eps,\n",
    "            optimizer=optim.SGD,\n",
    "            loss_fn = nn.MSELoss\n",
    "        )\n",
    "        ep_count, steps = cartpole_solver.learn()\n",
    "        cartpole_solver.close()\n",
    "        ax.plot(steps, label=f\"lr={lr} mem={mem}\")\n",
    "plt.legend()\n",
    "plt.title(f\"SGD_MSEloss_lr-{lr_space}_mem-{mem_space}_{eps}eps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "lr_space = [ 0.1]\n",
    "mem_space = [1000,10000, 100000]\n",
    "eps = 500\n",
    "for lr in lr_space:\n",
    "    for mem in mem_space:\n",
    "        cartpole_solver = CPSolver(\n",
    "            episodes=eps,\n",
    "            memory=mem,\n",
    "            gamma=0.8,\n",
    "            lr=lr,\n",
    "            batch_size=32,\n",
    "            eps_start=0.9,\n",
    "            eps_end=0.05,\n",
    "            eps_decay=200,\n",
    "            output=False,\n",
    "            render=True,\n",
    "            render_step=eps,\n",
    "            optimizer=optim.SGD,\n",
    "            loss_fn = nn.MSELoss\n",
    "        )\n",
    "        ep_count, steps = cartpole_solver.learn()\n",
    "        cartpole_solver.close()\n",
    "        ax.plot(steps, label=f\"lr={lr} mem={mem}\")\n",
    "plt.legend()\n",
    "plt.title(f\"SGD_MSEloss_lr-{lr_space}_mem-{mem_space}_{eps}eps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "lr_space = [ 0.01]\n",
    "mem_space = [1000,10000, 100000]\n",
    "eps = 500\n",
    "for lr in lr_space:\n",
    "    for mem in mem_space:\n",
    "        cartpole_solver = CPSolver(\n",
    "            episodes=eps,\n",
    "            memory=mem,\n",
    "            gamma=0.8,\n",
    "            lr=lr,\n",
    "            batch_size=32,\n",
    "            eps_start=0.9,\n",
    "            eps_end=0.05,\n",
    "            eps_decay=200,\n",
    "            output=False,\n",
    "            render=True,\n",
    "            render_step=eps,\n",
    "            optimizer=optim.SGD,\n",
    "            loss_fn = nn.MSELoss\n",
    "        )\n",
    "        ep_count, steps = cartpole_solver.learn()\n",
    "        cartpole_solver.close()\n",
    "        ax.plot(steps, label=f\"lr={lr} mem={mem}\")\n",
    "plt.legend()\n",
    "plt.title(f\"SGD_MSEloss_lr-{lr_space}_mem-{mem_space}_{eps}eps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "lr_space = [ 0.001]\n",
    "mem_space = [1000,10000, 100000]\n",
    "eps = 500\n",
    "for lr in lr_space:\n",
    "    for mem in mem_space:\n",
    "        cartpole_solver = CPSolver(\n",
    "            episodes=eps,\n",
    "            memory=mem,\n",
    "            gamma=0.8,\n",
    "            lr=lr,\n",
    "            batch_size=32,\n",
    "            eps_start=0.9,\n",
    "            eps_end=0.05,\n",
    "            eps_decay=200,\n",
    "            output=False,\n",
    "            render=True,\n",
    "            render_step=eps,\n",
    "            optimizer=optim.SGD,\n",
    "            loss_fn = nn.MSELoss\n",
    "        )\n",
    "        ep_count, steps = cartpole_solver.learn()\n",
    "        cartpole_solver.close()\n",
    "        ax.plot(steps, label=f\"lr={lr} mem={mem}\")\n",
    "plt.legend()\n",
    "plt.title(f\"SGD_MSEloss_lr-{lr_space}_mem-{mem_space}_{eps}eps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "lr_space = [ 0.01]\n",
    "mem_space = [1000,10000, 100000]\n",
    "eps = 250\n",
    "for lr in lr_space:\n",
    "    for mem in mem_space:\n",
    "        cartpole_solver = CPSolver(\n",
    "            episodes=eps,\n",
    "            memory=mem,\n",
    "            gamma=0.8,\n",
    "            lr=lr,\n",
    "            batch_size=32,\n",
    "            eps_start=0.9,\n",
    "            eps_end=0.05,\n",
    "            eps_decay=200,\n",
    "            output=False,\n",
    "            render=True,\n",
    "            render_step=eps,\n",
    "            optimizer=optim.SGD,\n",
    "            loss_fn = nn.MSELoss\n",
    "        )\n",
    "        ep_count, steps = cartpole_solver.learn()\n",
    "        cartpole_solver.close()\n",
    "        ax.plot(steps, label=f\"lr={lr} mem={mem}\")\n",
    "plt.legend()\n",
    "plt.title(f\"SGD_MSEloss_lr-{lr_space}_mem-{mem_space}_{eps}eps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Eps greedy exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "lr_space = [ 0.01]\n",
    "mem_space = [10000]\n",
    "eps = 500\n",
    "for lr in lr_space:\n",
    "    for mem in mem_space:\n",
    "        cartpole_solver = CPSolver(\n",
    "            episodes=eps,\n",
    "            memory=mem,\n",
    "            gamma=0.8,\n",
    "            lr=lr,\n",
    "            batch_size=32,\n",
    "\n",
    "            eps_start=0.9,\n",
    "            eps_end=0.05,\n",
    "            eps_decay=200,\n",
    "            output=False,\n",
    "            render=True,\n",
    "            render_step=100,\n",
    "            optimizer=optim.SGD,\n",
    "            loss_fn = nn.MSELoss\n",
    "        )\n",
    "        ep_count, steps = cartpole_solver.learn()\n",
    "        cartpole_solver.close()\n",
    "        ax.plot(steps, label=f\"lr={lr} mem={mem}\")\n",
    "plt.legend()\n",
    "plt.title(f\"SGD_MSEloss_lr-{lr_space}_mem-{mem_space}_{eps}eps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "lr_space = [ 0.01]\n",
    "mem_space = [10000]\n",
    "eps = 500\n",
    "for lr in lr_space:\n",
    "    for mem in mem_space:\n",
    "        cartpole_solver = CPSolver(\n",
    "            episodes=eps,\n",
    "            memory=mem,\n",
    "            gamma=0.8,\n",
    "            lr=lr,\n",
    "            batch_size=32,\n",
    "\n",
    "            eps_start=0.9,\n",
    "            eps_end=0.05,\n",
    "            eps_decay=100,\n",
    "            output=False,\n",
    "            render=True,\n",
    "            render_step=100,\n",
    "            optimizer=optim.SGD,\n",
    "            loss_fn = nn.MSELoss\n",
    "        )\n",
    "        ep_count, steps = cartpole_solver.learn()\n",
    "        cartpole_solver.close()\n",
    "        ax.plot(steps, label=f\"lr={lr} mem={mem}\")\n",
    "plt.legend()\n",
    "plt.title(f\"SGD_MSEloss_lr-{lr_space}_mem-{mem_space}_{eps}eps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "lr_space = [ 0.01]\n",
    "mem_space = [10000]\n",
    "eps = 500\n",
    "for lr in lr_space:\n",
    "    for mem in mem_space:\n",
    "        cartpole_solver = CPSolver(\n",
    "            episodes=eps,\n",
    "            memory=mem,\n",
    "            gamma=0.8,\n",
    "            lr=lr,\n",
    "            batch_size=32,\n",
    "\n",
    "            eps_start=0.5,\n",
    "            eps_end=0.05,\n",
    "            eps_decay=100,\n",
    "            output=False,\n",
    "            render=True,\n",
    "            render_step=100,\n",
    "            optimizer=optim.SGD,\n",
    "            loss_fn = nn.MSELoss\n",
    "        )\n",
    "        ep_count, steps = cartpole_solver.learn()\n",
    "        cartpole_solver.close()\n",
    "        ax.plot(steps, label=f\"lr={lr} mem={mem}\")\n",
    "plt.legend()\n",
    "plt.title(f\"SGD_MSEloss_lr-{lr_space}_mem-{mem_space}_{eps}eps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "lr_space = [ 0.01]\n",
    "mem_space = [10000]\n",
    "eps = 500\n",
    "for lr in lr_space:\n",
    "    for mem in mem_space:\n",
    "        cartpole_solver = CPSolver(\n",
    "            episodes=eps,\n",
    "            memory=mem,\n",
    "            gamma=0.8,\n",
    "            lr=lr,\n",
    "            batch_size=32,\n",
    "\n",
    "            eps_start=0.1,\n",
    "            eps_end=0.01,\n",
    "            eps_decay=100,\n",
    "            output=False,\n",
    "            render=True,\n",
    "            render_step=100,\n",
    "            optimizer=optim.SGD,\n",
    "            loss_fn = nn.MSELoss\n",
    "        )\n",
    "        ep_count, steps = cartpole_solver.learn()\n",
    "        cartpole_solver.close()\n",
    "        ax.plot(steps, label=f\"lr={lr} mem={mem}\")\n",
    "plt.legend()\n",
    "plt.title(f\"SGD_MSEloss_lr-{lr_space}_mem-{mem_space}_{eps}eps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Form of the decaying epsilon greedy algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def decaying_eps_greedy(steps_done, start, end, decay):\n",
    "        return end + (start - end) * np.exp( -1.0 * steps_done / decay)\n",
    "start = 0.9\n",
    "end = 0.05\n",
    "decay_const = 100\n",
    "x = np.arange(0,501)\n",
    "y = decaying_eps_greedy(x, start, end, decay_const)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x,y,c='k',label=\"decaying $\\epsilon$-greedy\")\n",
    "ax.axvline(decay_const,c='y',ls='--',label= f'1xdecay_const={decay_const}')\n",
    "ax.axvline(decay_const*3,c='r',ls='--',label= f'3xdecay_const={decay_const*3}')\n",
    "ax.axhline(end,c='g',ls='--',label=f'endpoint={end}')\n",
    "\n",
    "\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Review of first attempt at Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Done well\n",
    "- training is successful. We show increase in performance when using a simple fully connected network, and deepQ learning\n",
    "- training output is plotted\n",
    "- implemented options for actually viewing the performance during training at a variety of points\n",
    "- code is extensible to different Networks provided they satisfy the input/output requirements\n",
    "\n",
    "#### Points of difficulty\n",
    "- need better understanding of how the maxQ step of the optimization works. perhaps a short writeup?\n",
    "- model could be generalized as an excercise:\n",
    "    - to take alternative inputs eg. the image of the system rather than the system state as input.\n",
    "    - to be applied to any of the other openAI gym environments with arbitrary action space\n",
    "\n",
    "#### Thinking points for next project\n",
    "- work through the fiddly tensor arithmatic that goes in to taking the raw input data of whatever type in to batches for the model.\n",
    "- build the model architechture myself instead of following tutorials. This is a good excercise, and can always check in with max for a sanity check.\n",
    "- build in some way for saving a model, and then using it for repeat inference to compare models, \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering for presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "lr_space = [0.01]\n",
    "mem_space = [10000]\n",
    "eps = 500\n",
    "for lr in lr_space:\n",
    "    for mem in mem_space:\n",
    "        cartpole_solver = CPSolver(\n",
    "            episodes=eps,\n",
    "            memory=mem,\n",
    "            gamma=0.8,\n",
    "            lr=lr,\n",
    "            batch_size=32,\n",
    "            eps_start=0.9,\n",
    "            eps_end=0.05,\n",
    "            eps_decay=200,\n",
    "            output=True,\n",
    "            render=False,\n",
    "            render_step=100,\n",
    "            optimizer=optim.SGD,\n",
    "            loss_fn = nn.MSELoss\n",
    "        )\n",
    "        ep_count, steps = cartpole_solver.learn()\n",
    "        cartpole_solver.close()\n",
    "        ax.plot(steps, label=f\"lr={lr} mem={mem}\")\n",
    "plt.legend()\n",
    "plt.title(f\"SGD_MSEloss_lr-{lr_space}_mem-{mem_space}_{eps}eps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
