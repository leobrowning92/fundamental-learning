{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Reinforcement-Learning-Example\" data-toc-modified-id=\"Reinforcement-Learning-Example-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Reinforcement Learning Example</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model,-network-and-memory-definitions\" data-toc-modified-id=\"Model,-network-and-memory-definitions-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Model, network and memory definitions</a></span></li><li><span><a href=\"#Example-with-defaults\" data-toc-modified-id=\"Example-with-defaults-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Example with defaults</a></span><ul class=\"toc-item\"><li><span><a href=\"#Example-comparing-hyperparameters\" data-toc-modified-id=\"Example-comparing-hyperparameters-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Example comparing hyperparameters</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing that I have changed from my eploration of RL \n",
    "is that I have added a little bit of a docstring to my base class CPSolver\n",
    "and removed the many many cells of hyperparameter searching.\n",
    "\n",
    "If you want to read more, do some googling. But also check out this paper https://arxiv.org/abs/1312.5602\n",
    "\n",
    "[Disclaimer] It was never meant to see the light of day\n",
    "\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses Python 3.7\n",
    "# quick and dirty way to make sure you have the right packages \n",
    "# uncomment the next line and run this cell\n",
    "# ! pip install jupyter gym torch matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import gym\n",
    "import attr\n",
    "import random\n",
    "import math\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pdb\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, network and memory definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\"Experience\", (\"state\", \"action\", \"reward\", \"next_state\"))\n",
    "\n",
    "\n",
    "@attr.s\n",
    "class ReplayMemory(object):\n",
    "    capacity = attr.ib()\n",
    "    memory = []\n",
    "    position = 0\n",
    "\n",
    "    def push(self, transition):\n",
    "        \"\"\"adds experiences to the memory buffer\"\"\"\n",
    "        self.memory.append(transition)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    def __repr__(self):\n",
    "        return str(self.capacity)\n",
    "\n",
    "\n",
    "class CPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(4, 200)\n",
    "        # self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xb = x.view(-1, 4)\n",
    "        xb = F.relu(self.fc1(xb))\n",
    "        # xb = F.relu(self.fc2(xb))\n",
    "        xb = self.fc3(xb)\n",
    "        return xb.view(-1, xb.size(1))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class CPSolver(object):\n",
    "    \"\"\" \n",
    "    This is the base class that manages the whole RL pipeline.\n",
    "    This was written for my own understanding, but it runs and \n",
    "    hopefully provides a bit of insight.\n",
    "\n",
    "    Args:\n",
    "        episodes (int): Number of runs of the game (up to end) to perform\n",
    "        memory (int): Number of memories in the memory buffer\n",
    "        gamma (float): Parameter in the loss function determining the \n",
    "            fractional impact of actions. between 0 and 1\n",
    "        lr (float): The learning rate of the gradient descent when training the network\n",
    "        batch_size (int): Number of memories passed in for training simulataneously\n",
    "        eps_start (float): Starting fraction of true random decisions\n",
    "        eps_end (float): Final fraction of true random decisions\n",
    "        eps_decay (int): Exponential decay constant for the proportion of\n",
    "            random decisions in episodes. See self.eps_greedy for \n",
    "            implementation of decaying epsilon greedy strategy\n",
    "        optimizer (torch optimizer): SGD = stochastic gradient descent. could use others\n",
    "        loss_fn (torch loss): MSE = Mean square error loss\n",
    "        render (bool): Whether you want to see the game run\n",
    "        render_step (int): render the game every x steps\n",
    "        output = False: For saving video files. requires ffmpeg\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        episodes=300,\n",
    "        memory=10000,\n",
    "        gamma=0.8,\n",
    "        lr=0.01,\n",
    "        batch_size=32,\n",
    "        eps_start=0.9,\n",
    "        eps_end=0.01,\n",
    "        eps_decay=100,\n",
    "        optimizer=optim.SGD,\n",
    "        loss_fn=nn.MSELoss,\n",
    "        render = True,\n",
    "        render_step = 100,\n",
    "        output = False\n",
    "    ):\n",
    "\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_decay = eps_decay\n",
    "        self.episodes = episodes\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.memory_size = memory\n",
    "        \n",
    "        self.memory = ReplayMemory(memory)\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        self.model = CPNet()\n",
    "        self.optimizer = optimizer(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn=loss_fn()\n",
    "        \n",
    "        self.render = render\n",
    "        self.render_step = render_step\n",
    "        if output:\n",
    "            # enables video file output\n",
    "            self.env = gym.wrappers.Monitor(self.env, f'./RL_vids/{str(time())}/',video_callable=self.render_check)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def render_check(self, step):\n",
    "        if step==0:\n",
    "            return True\n",
    "        else:\n",
    "            return (step+1)%self.render_step==0\n",
    "    \n",
    "    def eps_threshold(self, steps_done):\n",
    "        return self.eps_end + (self.eps_start - self.eps_end) * math.exp(\n",
    "            -1.0 * steps_done / self.eps_decay\n",
    "        )\n",
    "\n",
    "    def select_action(self, state, steps_done):\n",
    "        \"\"\"Selects the best action using the model\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # model predicts highest predicted reward\n",
    "            prediction = self.model(state)\n",
    "            # selects the action with the highest predicted probability\n",
    "            action = prediction.data.max(1)[1].view(1, 1)\n",
    "        return action\n",
    "            \n",
    "        \n",
    "    def eps_greedy(self, state, steps_done):\n",
    "        if random.random() > self.eps_threshold(steps_done):\n",
    "            return self.select_action(state, steps_done)\n",
    "        else:\n",
    "            return torch.tensor([[random.choice([0, 1])]])\n",
    "        \n",
    "\n",
    "    def optimize_model(self):\n",
    "        # lol\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        # print(transitions)\n",
    "\n",
    "        batch_state, batch_action, batch_reward, batch_next_state = zip(*transitions)\n",
    "\n",
    "        batch_state = torch.cat(batch_state)\n",
    "        batch_action = torch.cat(batch_action)\n",
    "        batch_reward = torch.cat(batch_reward)\n",
    "        batch_next_state = torch.cat(batch_next_state)\n",
    "        # print(f'batch_reward:{batch_reward}')\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        # The network returns probabilities\n",
    "        # The probs corresponding to the actions taken are selected\n",
    "        current_q_values = self.model(batch_state).gather(1, batch_action).view(-1)\n",
    "        # best probabilities possible from the next state\n",
    "        max_next_q_values = self.model(batch_next_state).detach().max(1)[0]\n",
    "        \n",
    "        expected_q_values = batch_reward + (self.gamma * max_next_q_values)\n",
    "        \n",
    "        # loss is measured from error between current and Best expected Q values.\n",
    "        loss = self.loss_fn(current_q_values, expected_q_values)\n",
    "        # backpropagation of loss to NN\n",
    "        loss.backward()\n",
    "        # print(f'ep:{episode:03d}-step:{i:03d}-loss:{loss:0.4f}')\n",
    "        # print(current_q_values.mean().item(), expected_q_values.mean().item())\n",
    "        # print(optimizer.param_groups[0]['params'][0].grad.mean().item(),\n",
    "        #       optimizer.param_groups[0]['params'][0].grad.var().item())\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def learn(self):\n",
    "        steps_done = 0\n",
    "        ep_count = []\n",
    "        step_count = []\n",
    "        \n",
    "        for episode in range(self.episodes):\n",
    "            state = self.env.reset()\n",
    "            for i in count():\n",
    "                if self.render and (episode+1) % self.render_step ==0:\n",
    "                    if i==0:\n",
    "                        print(f'lr:{self.lr}_mem:{self.memory_size}_Episode:{episode+1}')\n",
    "                    self.env.render()\n",
    "                action = self.eps_greedy(torch.FloatTensor([state]), steps_done)\n",
    "                steps_done += 1\n",
    "                next_state, reward, done, info = self.env.step(action[0, 0].item())\n",
    "                if done:\n",
    "                    reward = -1\n",
    "                self.memory.push(\n",
    "                    (\n",
    "                        torch.FloatTensor(state),\n",
    "                        action,\n",
    "                        torch.FloatTensor([reward]),\n",
    "                        torch.FloatTensor(next_state),\n",
    "                    )\n",
    "                )\n",
    "                # Only train if there are enough memories to produce a full batch\n",
    "                if len(self.memory) >= self.batch_size:\n",
    "                    self.optimize_model()\n",
    "                    \n",
    "                state = next_state\n",
    "                if done:\n",
    "                    ep_count.append(episode)\n",
    "                    step_count.append(i)\n",
    "                    break\n",
    "        return ep_count, step_count\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "cartpole_solver = CPSolver()\n",
    "ep_count, steps = cartpole_solver.learn()\n",
    "cartpole_solver.close()\n",
    "ax.plot(steps, label=f\"lr={cartpole_solver.lr} mem={cartpole_solver.memory}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example comparing hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "lr_space = [1,0.1,0.01]\n",
    "for lr in lr_space:\n",
    "    cartpole_solver = CPSolver(lr=lr)\n",
    "    ep_count, steps = cartpole_solver.learn()\n",
    "    cartpole_solver.close()\n",
    "    ax.plot(steps, label=f\"lr={cartpole_solver.lr} mem={cartpole_solver.memory}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
