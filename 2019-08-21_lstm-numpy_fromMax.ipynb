{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code from Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def __call__(self, *args):\n",
    "        return self.forward(*args)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        raise NotImplementedError(\"Please overwrite.\")\n",
    "\n",
    "class Parameter(object):\n",
    "    def __init__(self, array):\n",
    "        self.array = array\n",
    "        self.grad = np.zeros_like(array)\n",
    "    \n",
    "    def reset_gradients(self):\n",
    "        self.grad = np.zeros_like(self.array)\n",
    "\n",
    "class Tanh(Module):\n",
    "    def forward(self, x):\n",
    "        return (np.exp(2 * x) - 1) / (np.exp(2 * x) + 1)\n",
    "\n",
    "class Sigmoid(Module):\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class LSTMCell(Module):\n",
    "    def __init__(self, input_size=10, hidden_size=128):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.W = Parameter(np.random.randn(input_size, 4 * hidden_size))\n",
    "        self.U = Parameter(np.random.randn(hidden_size, 4 * hidden_size))\n",
    "        self.bias = Parameter(np.random.randn(4 * hidden_size))\n",
    "\n",
    "        self.tanh = Tanh()\n",
    "        self.sigmoid = Sigmoid()\n",
    "        \n",
    "        self.backprop_cache = {}\n",
    "\n",
    "    def forward(self, x, h=None, c=None):\n",
    "        if h is None:\n",
    "            h = np.zeros((x.shape[0], self.hidden_size))\n",
    "        if c is None:\n",
    "            c = np.zeros((x.shape[0], self.hidden_size))\n",
    "            \n",
    "        at = np.dot(x, self.W.array) + np.dot(h, self.U.array) + self.bias.array\n",
    "\n",
    "        ft = self.sigmoid(at[:, :self.hidden_size])\n",
    "        it = self.sigmoid(at[:, self.hidden_size:(self.hidden_size*2)])\n",
    "        ot = self.sigmoid(at[:, (self.hidden_size*2):(self.hidden_size*3)])\n",
    "        zt = self.tanh(at[:, (self.hidden_size*3):])\n",
    "        ct = ft * c + it * zt\n",
    "        ht = ot * self.tanh(ct)\n",
    "        \n",
    "        self.backprop_cache = {\n",
    "            'x': x,\n",
    "            'W': self.W,\n",
    "            'U': self.U,\n",
    "            'bias': self.bias,\n",
    "            'h': h,\n",
    "            'c': c,\n",
    "            'ft': ft,\n",
    "            'it': it,\n",
    "            'ot': ot,\n",
    "            'ct': ct,\n",
    "            'zt': zt,\n",
    "            'ht': ht,\n",
    "            'at': at\n",
    "        }\n",
    "        return ht\n",
    "\n",
    "    def backward(self, above_y=None, above_dh=None, above_dc=None):\n",
    "        # TODO : make loss not just linear\n",
    "        d_err_net = -1\n",
    "        d_net_ct = tanh_backward(self.backprop_cache['ct']) * self.backprop_cache['ot'] * d_err_net\n",
    "        d_net_ot = self.backprop_cache['ct'] * d_err_net\n",
    "        d_ct_ft = self.backprop_cache['c'] * d_net_ct\n",
    "        d_ct_it = self.backprop_cache['zt'] * d_net_ct\n",
    "        d_ct_zt = self.backprop_cache['it'] * d_net_ct\n",
    "        d_ft_at = sig_backward(self.backprop_cache['at'][:, :self.hidden_size]) * d_ct_ft\n",
    "        d_it_at = sig_backward(self.backprop_cache['at'][:, self.hidden_size:(self.hidden_size*2)]) * d_ct_it\n",
    "        d_ot_at = sig_backward(self.backprop_cache['at'][:, (self.hidden_size*2):(self.hidden_size*3)]) * d_net_ot\n",
    "        d_zt_at = tanh_backward(self.backprop_cache['at'][:, (self.hidden_size*3):]) * d_ct_zt\n",
    "        d_at = np.concatenate([d_ft_at, d_it_at, d_ot_at, d_zt_at], axis=-1)\n",
    "        d_at_W = np.repeat(np.expand_dims(self.backprop_cache['x'], axis=-1), self.hidden_size * 4, axis=-1) * \\\n",
    "            np.expand_dims(d_at, axis=1)\n",
    "        d_at_U = np.repeat(np.expand_dims(self.backprop_cache['h'], axis=-1), self.hidden_size * 4, axis=-1) * \\\n",
    "            np.expand_dims(d_at, axis=1)\n",
    "        d_at_bias = self.backprop_cache['at'] * d_at\n",
    "        \n",
    "        # Update gradients\n",
    "        self.W.grad += d_at_W.mean(axis=0)\n",
    "        self.U.grad += d_at_U.mean(axis=0) \n",
    "        self.bias.grad += d_at_bias.mean(axis=0)\n",
    "\n",
    "        return d_at_W, d_at_U, d_at_bias\n",
    "\n",
    "    def numerical_grad(self, x, y, h=None, c=None, eps=1e-4, ix1=4, ix2=10):\n",
    "        # run forward\n",
    "        if h is None:\n",
    "            h = np.zeros((x.shape[0], self.hidden_size))\n",
    "        if c is None:\n",
    "            c = np.zeros((x.shape[0], self.hidden_size))\n",
    "\n",
    "        # test only one W\n",
    "        Wplus = self.W.array.copy()\n",
    "        Wplus[ix1, ix2] += eps\n",
    "        at = np.dot(x, Wplus) + np.dot(h, self.U.array) + (self.bias.array)\n",
    "        ft = self.sigmoid(at[:, :self.hidden_size])\n",
    "        it = self.sigmoid(at[:, self.hidden_size:(self.hidden_size*2)])\n",
    "        ot = self.sigmoid(at[:, (self.hidden_size*2):(self.hidden_size*3)])\n",
    "        zt = self.tanh(at[:, (self.hidden_size*3):])\n",
    "        ct = ft * c + it * zt\n",
    "        ht = ot * self.tanh(ct)\n",
    "        # compute loss\n",
    "        loss_1 = np.sum(y - ht, axis=1)\n",
    "        \n",
    "        Wminus = self.W.array.copy()\n",
    "        Wminus[ix1, ix2] -= eps\n",
    "        at = np.dot(x, Wminus) + np.dot(h, self.U.array) + (self.bias.array)\n",
    "        ft = self.sigmoid(at[:, :self.hidden_size])\n",
    "        it = self.sigmoid(at[:, self.hidden_size:(self.hidden_size*2)])\n",
    "        ot = self.sigmoid(at[:, (self.hidden_size*2):(self.hidden_size*3)])\n",
    "        zt = self.tanh(at[:, (self.hidden_size*3):])\n",
    "        ct = ft * c + it * zt\n",
    "        ht = ot * self.tanh(ct)\n",
    "        # compute loss\n",
    "        loss_2 = np.sum(y - ht, axis=1)\n",
    "        \n",
    "        # compute gradient approx\n",
    "        return (loss_1 - loss_2) / (2 * eps)\n",
    "    \n",
    "    def reset_gradients(self):\n",
    "        self.W.reset_gradients()\n",
    "        self.U.reset_gradients()\n",
    "        self.bias.reset_gradients()\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sig_backward(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "def tanh_backward(x):\n",
    "    return 1 - (np.exp(x) - np.exp(-x)) ** 2 / (np.exp(x) + np.exp(-x)) ** 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTMCell()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Check on LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.ones((32, 10))\n",
    "yhat = lstm(x)\n",
    "lstm.reset_gradients()\n",
    "dw, du, db = lstm.backward(np.ones_like(yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lstm.W.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix1 = 9\n",
    "ix2 = 405\n",
    "print(dw[:, ix1, ix2])\n",
    "print(lstm.numerical_grad(x, np.ones_like(yhat), ix1=ix1, ix2=ix2, eps=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
