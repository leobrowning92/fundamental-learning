{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#word-vector-exploration\" data-toc-modified-id=\"word-vector-exploration-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>word vector exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-manual-exploration-of-GloVe-vectors\" data-toc-modified-id=\"First-manual-exploration-of-GloVe-vectors-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>First manual exploration of GloVe vectors</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Cosine-similarity\" data-toc-modified-id=\"Cosine-similarity-1.1.0.1\"><span class=\"toc-item-num\">1.1.0.1&nbsp;&nbsp;</span>Cosine similarity</a></span></li></ul></li></ul></li><li><span><a href=\"#test-corpus-form-CS241-asignment-1\" data-toc-modified-id=\"test-corpus-form-CS241-asignment-1-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>test corpus form CS241 asignment 1</a></span></li><li><span><a href=\"#Straight-summed-corpus\" data-toc-modified-id=\"Straight-summed-corpus-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Straight summed corpus</a></span></li><li><span><a href=\"#tf-idf-exploration\" data-toc-modified-id=\"tf-idf-exploration-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>tf-idf exploration</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word vector exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## First manual exploration of GloVe vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glove data from https://nlp.stanford.edu/projects/glove/ we will start with the 50 dimensional vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname =\"/Users/leo.browning/ml_data/glove.6B/glove.6B.50d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are 400K embeddings, we can use this to chunk things into maneagable bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_gloveline(path):\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            cleanline = line.strip().split(' ')\n",
    "            yield cleanline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_array = np.array([line for line in split_gloveline(fname)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=glove_array[:,0]\n",
    "embeddings=glove_array[:,1:].astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {word:index for word,index in zip(words,np.arange(len(words)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_norms = np.linalg.norm(embeddings,axis=1).reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_embeddings=np.divide(embeddings,embedding_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embed = normalized_embeddings[word_index[\"test\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_embeddings[word_index[\"test\"]].dot(test_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sim_matrix = normalized_embeddings.dot(normalized_embeddings.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embed.dot(normalized_embeddings.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_index  = np.argsort(test_embed.dot(normalized_embeddings.T))[-10:]\n",
    "top_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[top_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_closest_words(vec,embeddings=normalized_embeddings,words=words, num =10):\n",
    "    indices = np.argsort(vec.dot(embeddings.T))[-num:]\n",
    "    return words[indices][::-1]\n",
    "def embedding_of(word,word_index=word_index,embeddings=normalized_embeddings):\n",
    "    return embeddings[word_index[word]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_closest_words(test_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Cosine similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$ \\text{similarity} =\\frac{\\vec A\\cdot\\vec B}{\\| A\\|  \\|B\\| }$$\n",
    "\n",
    "quote from https://scikit-learn.org/stable/modules/metrics.html#cosine-similarity :\n",
    "\n",
    "cosine_similarity computes the L2-normalized dot product of vectors. That is, if  and  are row vectors, their cosine similarity  is defined as:\n",
    "\n",
    "This is called cosine similarity, because Euclidean (L2) normalization projects the vectors onto the unit sphere, and their dot product is then the cosine of the angle between the points denoted by the vectors.\n",
    "\n",
    "This [cosine similarity] kernel is a popular choice for computing the similarity of documents represented as tf-idf vectors. cosine_similarity accepts scipy.sparse matrices. (Note that the tf-idf functionality in sklearn.feature_extraction.text can produce normalized vectors, in which case cosine_similarity is equivalent to linear_kernel, only slower.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test corpus form CS241 asignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info[0]==3\n",
    "assert sys.version_info[1] >= 5\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import reuters,stopwords\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy as sp\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_corpus(category=\"crude\"):\n",
    "    return reuters.raw(category=\"crude\")\n",
    "\n",
    "def process_doc(doc, v=False):\n",
    "    reject =set(stopwords.words('english'))|set(string.punctuation)\n",
    "    doc1 = [w.lower() for w in doc]\n",
    "    doc2 = [w for w in doc1 if w not in reject]\n",
    "    doc2 = [w for w in doc2 if len(w)>1]\n",
    "    doc3 = [w for w in doc2 if (w in word_index)]\n",
    "    doc4 = [w for w in doc3 if not(re.search('\\d+',w))]\n",
    "    if v and len(doc4)==0:\n",
    "        print(f'{len(doc)} initially')\n",
    "        print(f'{len(doc1)} words after lower')\n",
    "        print(f'{len(doc2)} words after stopwords/punctuation')\n",
    "        print(f'{len(doc2)} words after len=1 removal')\n",
    "        print(f'{len(doc3)} words after check in index')\n",
    "        print(f'{len(doc4)} words after number removal')\n",
    "    return doc4\n",
    "                                \n",
    "\n",
    "\n",
    "\n",
    "def read_corpus(category=\"crude\",v=False):\n",
    "    \"\"\" Read files from the specified Reuter's category.\n",
    "        Params:\n",
    "            category (string): category name\n",
    "        Return:\n",
    "            list of lists, with words from each of the processed files\n",
    "    \"\"\"\n",
    "    files = reuters.fileids(category)\n",
    "    corpus_words = [[w for w in list(reuters.words(f))] for f in files ]\n",
    "    corpus = [process_doc(doc,v=v) for doc in corpus_words]\n",
    "    return corpus,[' '.join(doc) for doc in corpus_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus,raw_corpus = read_corpus(category='crude',v=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(raw_corpus[0], compact=True, width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc=corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_indices = np.array([word_index.get(word,None) for word in test_doc])\n",
    "test_doc_indices = np.array([index for index in test_doc_indices if index is not None])\n",
    "test_doc_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsum = np.sum(normalized_embeddings[test_doc_indices], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_closest_words(docsum,normalized_embeddings,words,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This rightly shows that the sum reflects the most common words found in that document, which are mostly useless words. \n",
    "\n",
    "Need to try searching the corpus to see if differentiating words are still captured, ie searching for ocean should ideally cut through the 'the' 'a' crap.\n",
    "\n",
    "alternatively need to weight the vectors in the sum according to their frequency of occurrence, see tf-idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Straight summed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, raw_corpus = read_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,doc in enumerate(corpus):\n",
    "    if not(doc):\n",
    "        print(raw_corpus[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_embeddings(doc,word_index=word_index,embeddings=normalized_embeddings):\n",
    "    indices = [word_index.get(word,None) for word in doc]\n",
    "    cleaned_indices = [index for index in indices if index is not None]\n",
    "    if not(cleaned_indices):\n",
    "        print(doc)\n",
    "    embeddings = embeddings[np.array(cleaned_indices)]\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_corpus = np.array([doc_embeddings(doc) for doc in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_corpus = np.array([np.sum(doc,axis=0) for doc in embedded_corpus])\n",
    "summed_norms =np.linalg.norm(summed_corpus,axis=1).reshape((-1,1))\n",
    "summed_normalized_corpus=np.divide(summed_corpus,summed_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_normalized_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(word, embedded_corpus=summed_normalized_corpus, text_corpus=corpus, num_display=5,v=False):\n",
    "    wordvec = embedding_of(word)\n",
    "    similarity =wordvec.dot(embedded_corpus.T)\n",
    "    indices = np.argsort(similarity)[:-num_display-1:-1]\n",
    "    scores = similarity[indices]\n",
    "    if v:\n",
    "        for i,index in enumerate(indices):\n",
    "            print(f'==================================================')\n",
    "            print(f'{i+1}: score = {scores[i]} document #{index}')\n",
    "            print(f'==================================================')\n",
    "            if v>1:\n",
    "                pprint(' '.join(corpus[i]), compact=True, width=100)  \n",
    "    return np.array(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"flower\",num_display=2,v=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths = np.array([len(doc) for doc in embedded_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths[search(\"flower\",num_display=10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"japan\",num_display=1,v=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf stands for term frequency inverse document frequency.\n",
    "\n",
    "$$ \\text{tfidf}(t,d,D)=\\text{tf}(t,d)\\cdot \\text{idf}(t,D) $$\n",
    "\n",
    "where $t\\in d$ is the term, $d\\in D$ is the document in the corpus $D$.\n",
    "\n",
    "we will use $\\text{tf}(t,d) = f_{t,d}/|d|$ as the term freqency which is the count of a term in a document $f_{t,d}$ normalized for document length, and then using a log scaled inverse of the document frequency $\\text{idf}{t,D} = \\log \\left[ \\frac{N}{1+|\\{d \\in D : t \\in d\\}|} \\right]$ to give:\n",
    "\n",
    "$$ \\text{tfidf}(t,d,D)=\\frac{f_{t,d}}{|d|}\\cdot \\log \\left[ \\frac{N}{1+|\\{d \\in D : t \\in d\\}|} \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(term,corpus):\n",
    "    N=len(corpus)\n",
    "    indoc =sum([1 for doc in corpus if term in doc ])\n",
    "    # +1 to remove divbyzero errors\n",
    "    return np.log(N/(indoc+1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_idf = {word:idf(word,corpus) for word in set(corpus[0]).union(*corpus[1:])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(term,doc,corpus_idf=corpus_idf):\n",
    "    tf = doc.count(term)/len(doc)\n",
    "    idf = corpus_idf[term]\n",
    "    return tf*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordset in corpus\n",
    "len(set(corpus[0]).union(*corpus[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0].count('long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf('long',corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf('japan',corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_doc_embeddings(doc,corpus_idf=corpus_idf,word_index=word_index,embeddings=normalized_embeddings):\n",
    "    scaled_embeddings=[]\n",
    "    for word in doc:\n",
    "        if word in word_index:\n",
    "            index=word_index.get(word)\n",
    "            tfidf_scaling=tfidf(word,doc)\n",
    "            scaled_embeddings.append(embeddings[index]*tfidf_scaling)\n",
    "    return np.array(scaled_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_embedded_corpus = [tfidf_doc_embeddings(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_doc_embeddings(corpus[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_summed_corpus = np.array([np.sum(doc,axis=0) for doc in tfidf_embedded_corpus])\n",
    "tfidf_summed_norms =np.linalg.norm(tfidf_summed_corpus,axis=1).reshape((-1,1))\n",
    "tfidf_summed_normalized_corpus=np.divide(tfidf_summed_corpus,tfidf_summed_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_summed_normalized_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normal vs tfidf search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normal search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"japan\",num_display=5,v=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"japan\",num_display=2,v=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tfidf search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"japan\",num_display=5,v=1,embedded_corpus=tfidf_summed_normalized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths = np.array([len(doc) for doc in embedded_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"japan\",num_display=2,embedded_corpus=tfidf_summed_normalized_corpus,v=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
