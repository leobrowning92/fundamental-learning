{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019-10-03_fundamentallearning_pytorchCharRNN_generateName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see here https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import unicodedata\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger('DeepPockets')\n",
    "\n",
    "# the base info stream\n",
    "formatter1 = logging.Formatter('[%(asctime)s][%(levelname)s][%(name)s][%(funcName)s] %(message)s')\n",
    "stream_handler1 = logging.StreamHandler()\n",
    "stream_handler1.setLevel('DEBUG') # will print for anythin, provided\n",
    "stream_handler1.setFormatter(formatter1)\n",
    "\n",
    "# the base info stream\n",
    "formatter2 = logging.Formatter('[%(asctime)s][%(levelname)s][%(name)s] %(message)s')\n",
    "stream_handler2 = logging.StreamHandler()\n",
    "stream_handler2.setLevel('INFO') # will print for anythin, provided\n",
    "stream_handler2.setFormatter(formatter2)\n",
    "\n",
    "log.addHandler(stream_handler1)\n",
    "log.addHandler(stream_handler2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.setLevel('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-03 21:38:28,720][INFO][DeepPockets][<module>] hello\n",
      "[2019-12-03 21:38:28,720][INFO][DeepPockets] hello\n"
     ]
    }
   ],
   "source": [
    "log.info('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.debug('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://download.pytorch.org/tutorial/data.zip'\n",
    "if not os.path.isfile('data/pytorch_tutorial/data.zip'):\n",
    "    r = requests.get(url)\n",
    "    with open('data/pytorch_tutorial/data.zip', 'wb') as f:\n",
    "        f.write(r.content)\n",
    "fnames = glob.glob('data/pytorch_tutorial/data/names/*.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and organising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters = string.ascii_letters + \" .,;'-\"\n",
    "n_letters = len(all_letters)+1 #accounts for the EOS character\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "def clean_names(name):\n",
    "    return unicode_to_ascii(name.strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spanish', 'German', 'Polish', 'Russian', 'Chinese', 'Portuguese', 'Japanese', 'French', 'English', 'Korean', 'Irish', 'Arabic', 'Vietnamese', 'Dutch', 'Italian', 'Scottish', 'Czech', 'Greek'] 18\n"
     ]
    }
   ],
   "source": [
    "# dict matching category (language) to list of names\n",
    "category_lines = {}\n",
    "# list of all languages\n",
    "all_categories = []\n",
    "# number of categories\n",
    "n_categories = 0\n",
    "for fname in fnames:\n",
    "    with open(fname, 'r') as f:\n",
    "        category = os.path.basename(fname)[:-4]\n",
    "        names = [clean_names(line) for line in f]\n",
    "        all_categories.append(category)\n",
    "        category_lines[category] = names\n",
    "n_categories = len(all_categories)\n",
    "print(all_categories, n_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data to tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to code the words as one hot vectors of dimension [1,n_letters], the first dimension is the batch size (1)\n",
    "\n",
    "thus each word needs to be a [word_length,1,n_letters] tensor of one hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_to_tensor(l):\n",
    "    l_t = torch.zeros(1, n_letters)\n",
    "    l_t[0,all_letters.find(l)] = 1\n",
    "    return l_t\n",
    "    \n",
    "def category_tensor(category):\n",
    "    li = all_categories.index(category)\n",
    "    tensor = torch.zeros(1, n_categories)\n",
    "    tensor[0,li] = 1\n",
    "    return tensor\n",
    "\n",
    "def input_tensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for i,letter in enumerate(line):\n",
    "        tensor[i,0,all_letters.find(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "def target_tensor(line):\n",
    "    # all letter indices except first\n",
    "    letter_index = [all_letters.find(letter) for letter in line][1:]\n",
    "    # appends the EOF char, represented by the index number +1 from the end\n",
    "    # of all_letters\n",
    "    letter_index.append(n_letters - 1)\n",
    "    return torch.LongTensor(letter_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 59])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor('abc').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0, 51, 57, 58])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor('aaaZ-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_choice(l):\n",
    "    return l[random.randint(0,len(l)-1)]\n",
    "\n",
    "def random_example(subset=False):\n",
    "    category = subset if subset else random_choice(all_categories)\n",
    "    word = random_choice(category_lines[category])\n",
    "    cat_t = category_tensor(category)\n",
    "    in_t = input_tensor(word)\n",
    "    target_t = target_tensor(word)\n",
    "    return category, word,  cat_t, in_t, target_t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japanese Kuramochi torch.Size([1, 18]) torch.Size([9, 1, 59]) torch.Size([9])\n",
      "Russian Paidyshev torch.Size([1, 18]) torch.Size([9, 1, 59]) torch.Size([9])\n",
      "Portuguese Pereira torch.Size([1, 18]) torch.Size([7, 1, 59]) torch.Size([7])\n",
      "Russian Bagdasarov torch.Size([1, 18]) torch.Size([10, 1, 59]) torch.Size([10])\n",
      "Portuguese Simoes torch.Size([1, 18]) torch.Size([6, 1, 59]) torch.Size([6])\n",
      "Vietnamese Than torch.Size([1, 18]) torch.Size([4, 1, 59]) torch.Size([4])\n",
      "German Boehler torch.Size([1, 18]) torch.Size([7, 1, 59]) torch.Size([7])\n",
      "Polish Gajos torch.Size([1, 18]) torch.Size([5, 1, 59]) torch.Size([5])\n",
      "Irish O'Sullivan torch.Size([1, 18]) torch.Size([10, 1, 59]) torch.Size([10])\n",
      "Arabic Daher torch.Size([1, 18]) torch.Size([5, 1, 59]) torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    category, word,  cat_t, in_t, target_t  = random_example()\n",
    "    print(category, word,  cat_t.shape, in_t.shape, target_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_categories, data_size, hidden_size, output_size, gpu=False):\n",
    "        super(RNN,self).__init__()\n",
    "        self.combined_size = n_categories + data_size + hidden_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2o = nn.Linear(self.combined_size,output_size)\n",
    "        self.i2h = nn.Linear(self.combined_size,hidden_size)\n",
    "        self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.criterion = nn.NLLLoss()\n",
    "        self.gpu=gpu\n",
    "        if self.gpu:\n",
    "            self.cuda()\n",
    "    \n",
    "    def forward(self, category, x, last_hidden):\n",
    "        combined_data = torch.cat((category, x, last_hidden), 1)\n",
    "        output = self.i2o(combined_data)\n",
    "        hidden = self.i2h(combined_data)\n",
    "        out_combined = torch.cat((output, hidden), 1)\n",
    "        output = self.o2o(out_combined)\n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "    \n",
    "    \n",
    "    def train(self, cat_t,word_t,target_t,lr=0.0005):\n",
    "        hidden = self.init_hidden()\n",
    "        target_t.unsqueeze_(-1)\n",
    "        if self.gpu:\n",
    "            target_t = target_t.cuda()\n",
    "            cat_t = cat_t.cuda()\n",
    "            word_t = word_t.cuda()\n",
    "            hidden = hidden.cuda()\n",
    "\n",
    "        self.zero_grad()\n",
    "        loss = 0\n",
    "\n",
    "        for i in range(word_t.size()[0]):\n",
    "            output, hidden = self(cat_t, word_t[i],hidden)\n",
    "            l = self.criterion(output, target_t[i])\n",
    "            loss += l\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        for p in self.parameters():\n",
    "            p.data.add_(-lr,p.grad.data)\n",
    "        return output, loss.item()/word_t.size()[0]\n",
    "    \n",
    "    def generate(self, start = '', category='English'):\n",
    "        cat_t = category_tensor(category)\n",
    "        \n",
    "        hidden = self.init_hidden()\n",
    "        if self.gpu:\n",
    "            hidden = hidden.cuda()\n",
    "            cat_t = cat_t.cuda()\n",
    "        if not(start):\n",
    "            char = random_choice(all_letters)\n",
    "            word = char\n",
    "        else:\n",
    "            char = start[-1]\n",
    "            word = start\n",
    "        scores = 0\n",
    "        for i in range(20):\n",
    "            char_t = letter_to_tensor(char)\n",
    "            if self.gpu:\n",
    "                char_t = char_t.cuda()\n",
    "            output, hidden = self(cat_t, char_t,hidden)\n",
    "            prediction = category_from_output(output)\n",
    "            char = prediction[0]\n",
    "            scores += prediction[2]\n",
    "            if char == '#':\n",
    "                break\n",
    "            word = word + char\n",
    "            \n",
    "            \n",
    "        return (word, scores/len(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 128\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "rnn = RNN(n_categories, n_letters, n_hidden, n_letters)\n",
    "output, next_hidden = rnn(category_tensor('English'),letter_to_tensor('z'),hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 59]) torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "print(output.shape, next_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_from_output(out):\n",
    "    top_n, top_i = out.topk(1)\n",
    "    index = top_i[0].item()\n",
    "    score = top_n[0].item()\n",
    "    if index < len(all_letters):\n",
    "        category = all_letters[index]\n",
    "    else:\n",
    "        category ='#'\n",
    "        \n",
    "    return category, index, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('V', 47, -3.9471349716186523)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_from_output(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mVVVVVVVIIVDIIVVVVVVI', -3.7515671366737005)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.generate('','Japanese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_since(start):\n",
    "    now = time.time()\n",
    "    dt = now - start\n",
    "    mins = int(dt/60)\n",
    "    return f'{mins:>5} min {dt - mins*60:>5.2f} secs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 128\n",
    "hidden = torch.zeros(1, n_hidden)\n",
    "rnn = RNN(n_categories, n_letters, n_hidden, n_letters,gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 10000\n",
    "print_every = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:   200 | time:    0 min  1.12 secs | done:    2% | loss:    2.2513 | category:Japanese        generated: zaka                 score: -1.354\n",
      "iter:   400 | time:    0 min  2.25 secs | done:    4% | loss:    2.3097 | category:Japanese        generated: xakama               score: -1.172\n",
      "iter:   600 | time:    0 min  3.35 secs | done:    6% | loss:    2.2443 | category:Japanese        generated: Maka                 score: -1.323\n",
      "iter:   800 | time:    0 min  4.43 secs | done:    8% | loss:    2.2308 | category:Japanese        generated: Gaka                 score: -1.294\n",
      "iter:  1000 | time:    0 min  5.55 secs | done:   10% | loss:    2.2849 | category:Japanese        generated: Gaka                 score: -1.416\n",
      "iter:  1200 | time:    0 min  6.64 secs | done:   12% | loss:    2.2550 | category:Japanese        generated: goka                 score: -1.579\n",
      "iter:  1400 | time:    0 min  7.75 secs | done:   14% | loss:    2.2454 | category:Japanese        generated: Mokata               score: -1.253\n",
      "iter:  1600 | time:    0 min  8.87 secs | done:   16% | loss:    2.1764 | category:Japanese        generated: Maka                 score: -1.357\n",
      "iter:  1800 | time:    0 min  9.97 secs | done:   18% | loss:    2.2400 | category:Japanese        generated: Hakimo               score: -1.388\n",
      "iter:  2000 | time:    0 min 11.08 secs | done:   20% | loss:    2.2385 | category:Japanese        generated: laka                 score: -1.316\n",
      "iter:  2200 | time:    0 min 12.20 secs | done:   22% | loss:    2.2375 | category:Japanese        generated: laka                 score: -1.380\n",
      "iter:  2400 | time:    0 min 13.32 secs | done:   24% | loss:    2.1650 | category:Japanese        generated: hira                 score: -1.328\n",
      "iter:  2600 | time:    0 min 14.35 secs | done:   26% | loss:    2.2352 | category:Japanese        generated: Vaka                 score: -1.297\n",
      "iter:  2800 | time:    0 min 15.47 secs | done:   28% | loss:    2.2565 | category:Japanese        generated: taka                 score: -1.272\n",
      "iter:  3000 | time:    0 min 16.58 secs | done:   30% | loss:    2.2574 | category:Japanese        generated: maka                 score: -1.145\n",
      "iter:  3200 | time:    0 min 17.74 secs | done:   32% | loss:    2.1561 | category:Japanese        generated: caka                 score: -1.371\n",
      "iter:  3400 | time:    0 min 18.97 secs | done:   34% | loss:    2.1612 | category:Japanese        generated: Saka                 score: -1.362\n",
      "iter:  3600 | time:    0 min 20.05 secs | done:   36% | loss:    2.2658 | category:Japanese        generated: Iakamu               score: -1.395\n",
      "iter:  3800 | time:    0 min 21.15 secs | done:   38% | loss:    2.2045 | category:Japanese        generated: kaka                 score: -1.109\n",
      "iter:  4000 | time:    0 min 22.24 secs | done:   40% | loss:    2.2594 | category:Japanese        generated: paka                 score: -1.464\n",
      "iter:  4200 | time:    0 min 23.34 secs | done:   42% | loss:    2.1609 | category:Japanese        generated: Aaka                 score: -1.439\n",
      "iter:  4400 | time:    0 min 24.40 secs | done:   44% | loss:    2.2110 | category:Japanese        generated: paka                 score: -1.324\n",
      "iter:  4600 | time:    0 min 25.49 secs | done:   46% | loss:    2.2264 | category:Japanese        generated: Kakamo               score: -1.358\n",
      "iter:  4800 | time:    0 min 26.56 secs | done:   48% | loss:    2.1952 | category:Japanese        generated: Fatama               score: -1.211\n",
      "iter:  5000 | time:    0 min 27.67 secs | done:   50% | loss:    2.1765 | category:Japanese        generated: paki                 score: -1.362\n",
      "iter:  5200 | time:    0 min 28.78 secs | done:   52% | loss:    2.2153 | category:Japanese        generated: waka                 score: -1.129\n",
      "iter:  5400 | time:    0 min 29.90 secs | done:   54% | loss:    2.1835 | category:Japanese        generated: xaka                 score: -1.382\n",
      "iter:  5600 | time:    0 min 31.01 secs | done:   56% | loss:    2.2273 | category:Japanese        generated: vara                 score: -1.364\n",
      "iter:  5800 | time:    0 min 32.13 secs | done:   58% | loss:    2.1650 | category:Japanese        generated: xaki                 score: -1.359\n",
      "iter:  6000 | time:    0 min 33.23 secs | done:   60% | loss:    2.1580 | category:Japanese        generated: Dasa                 score: -1.533\n",
      "iter:  6200 | time:    0 min 34.34 secs | done:   62% | loss:    2.1812 | category:Japanese        generated: Maka                 score: -1.281\n",
      "iter:  6400 | time:    0 min 35.45 secs | done:   64% | loss:    2.1490 | category:Japanese        generated: Zosuno               score: -1.518\n",
      "iter:  6600 | time:    0 min 36.55 secs | done:   66% | loss:    2.1724 | category:Japanese        generated: akaka                score: -1.170\n",
      "iter:  6800 | time:    0 min 37.65 secs | done:   68% | loss:    2.1893 | category:Japanese        generated: Zaka                 score: -1.376\n",
      "iter:  7000 | time:    0 min 38.75 secs | done:   70% | loss:    2.1679 | category:Japanese        generated: Raka                 score: -1.195\n",
      "iter:  7200 | time:    0 min 39.87 secs | done:   72% | loss:    2.1791 | category:Japanese        generated: ;oshi                score: -1.261\n",
      "iter:  7400 | time:    0 min 40.98 secs | done:   74% | loss:    2.1624 | category:Japanese        generated: Baka                 score: -1.285\n",
      "iter:  7600 | time:    0 min 42.06 secs | done:   76% | loss:    2.2212 | category:Japanese        generated: Gaka                 score: -1.287\n",
      "iter:  7800 | time:    0 min 43.20 secs | done:   78% | loss:    2.1143 | category:Japanese        generated: ;aka                 score: -1.281\n",
      "iter:  8000 | time:    0 min 44.33 secs | done:   80% | loss:    2.1506 | category:Japanese        generated: Yaka                 score: -1.211\n",
      "iter:  8200 | time:    0 min 45.45 secs | done:   82% | loss:    2.1166 | category:Japanese        generated: takaki               score: -1.187\n",
      "iter:  8400 | time:    0 min 46.57 secs | done:   84% | loss:    2.0641 | category:Japanese        generated: qasaka               score: -1.363\n",
      "iter:  8600 | time:    0 min 47.66 secs | done:   86% | loss:    2.1811 | category:Japanese        generated: zaka                 score: -1.247\n",
      "iter:  8800 | time:    0 min 48.76 secs | done:   88% | loss:    2.0621 | category:Japanese        generated: gaka                 score: -1.315\n",
      "iter:  9000 | time:    0 min 49.87 secs | done:   90% | loss:    2.1400 | category:Japanese        generated: raki                 score: -1.236\n",
      "iter:  9200 | time:    0 min 50.96 secs | done:   92% | loss:    2.1759 | category:Japanese        generated: 'akana               score: -1.247\n",
      "iter:  9400 | time:    0 min 52.06 secs | done:   94% | loss:    2.0884 | category:Japanese        generated: laku                 score: -1.503\n",
      "iter:  9600 | time:    0 min 53.15 secs | done:   96% | loss:    2.0905 | category:Japanese        generated: waka                 score: -1.121\n",
      "iter:  9800 | time:    0 min 54.23 secs | done:   98% | loss:    2.1194 | category:Japanese        generated:  aka                 score: -1.223\n",
      "iter: 10000 | time:    0 min 55.36 secs | done:  100% | loss:    2.0253 | category:Japanese        generated: jaka                 score: -1.327\n"
     ]
    }
   ],
   "source": [
    "lang_restriction = 'Japanese'\n",
    "losses = []\n",
    "start = time.time()\n",
    "for i in range(n_iters):\n",
    "    cat,word,cat_t,word_t,target_t = random_example(lang_restriction)\n",
    "    output, loss = rnn.train(cat_t,word_t,target_t)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if (i+1)%print_every==0:\n",
    "        av_loss = sum(losses[-print_every:])/print_every\n",
    "        test_cat =lang_restriction\n",
    "        testgen = rnn.generate(category=test_cat)\n",
    "        test_string = (f'category:{test_cat:<15s} generated: {testgen[0]:<20s} score: {testgen[1]:<5.3f}')\n",
    "        print(f'iter:{i+1:>6} | time:{time_since(start)} | done:{(i+1)/n_iters*100:>5.0f}% | loss:{av_loss:>10.4f} | {test_string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run on batches? see https://towardsdatascience.com/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
