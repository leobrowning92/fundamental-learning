{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019-10-22_fundamentallearning_pytorchWordRNN_translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- follow this tutorial https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "- use reloading on training loop https://github.com/julvo/reloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is : cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import requests\n",
    "import unicodedata\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import pdb\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device is : {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get french english translations from https://www.manythings.org/anki/fra-eng.zip and put them in a folder `data/pytorch_tutorial/fra-eng/fra.txt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def timedelta_string(delta_time):\n",
    "    days = delta_time.days\n",
    "    hours = delta_time.seconds // 3600\n",
    "    minutes = delta_time.seconds % 3600 // 60\n",
    "    seconds = delta_time.seconds % 60 + delta_time.microseconds / 1e6\n",
    "    return f\"{days:3>}:{hours:02}:{minutes:02}:{seconds:05.2f}\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Processing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start and end tokens\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    \"\"\"Helper class to manage index <=> word translation\"\"\"\n",
    "    def __init__(self,name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.index2word = {SOS_token:'SOS', EOS_token:'EOS'}\n",
    "        self.word2count = {} # for rare words\n",
    "        self.n_words = 2\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.word2count[word] = 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD',s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    \n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    # adds a space before end of sentance punctuation\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s) \n",
    "    #replaces all other punctuation or unusual characters with spaces\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)     \n",
    "    return s\n",
    "    \n",
    "def sentence2indexlist(lang, s):\n",
    "    return [lang.word2index[w] for w in s.split(' ')]\n",
    "\n",
    "def sentence2tensor(lang,s):\n",
    "    indexes = sentence2indexlist(lang, s)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes ,dtype=torch.long, device=device).view(-1,1)\n",
    "\n",
    "def pair2tensors(l1,l2,pair):\n",
    "    input_tensor = sentence2tensor(l1,pair[0])\n",
    "    target_tensor = sentence2tensor(l2,pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter for short sentences that start with 'i am', 'he is' etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "ENG_PREFIXES = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def check_pair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and p[0].startswith(ENG_PREFIXES)\n",
    "def filter_pairs(pairs):\n",
    "    # file is english french, so must reverse when filtering\n",
    "    return [p[::-1] for p in pairs if check_pair(p)]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(lang1, lang2, simple_sentences = True):\n",
    "    print('Loading data')\n",
    "    with open(f'data/pytorch_tutorial/{lang1}-{lang2}/fra.txt') as f:\n",
    "        pairs = [[ normalizeString(s) for s in line.split('\\t')[:2]] for line in f]\n",
    "    print(f\"total sentance pairs in data : {len(pairs)}\")\n",
    "    print('filtering data')\n",
    "    if simple_sentences:\n",
    "        pairs = filter_pairs(pairs)\n",
    "        print(f\"Filtered sentance pairs in data : {len(pairs)}\")\n",
    "    print(random.choice(pairs))\n",
    "    l1 = Lang(lang1)\n",
    "    l2 = Lang(lang2)\n",
    "    \n",
    "    for p in pairs:\n",
    "        l1.add_sentence(p[0])\n",
    "        l2.add_sentence(p[1])\n",
    "    print(f'{l1.name} : {l1.n_words}')\n",
    "    print(f'{l2.name} : {l2.n_words}')\n",
    "    return l1, l2, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "total sentance pairs in data : 175623\n",
      "filtering data\n",
      "Filtered sentance pairs in data : 14213\n",
      "['je serai en retard .', 'i m going to be late .']\n",
      "fra : 5037\n",
      "eng : 3262\n"
     ]
    }
   ],
   "source": [
    "fra_lang, eng_lang, sentence_pairs = prepare_data('fra', 'eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['j ai ans .', 'i m .']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[2],\n",
       "         [3],\n",
       "         [4],\n",
       "         [5],\n",
       "         [1]], device='cuda:0'), tensor([[2],\n",
       "         [3],\n",
       "         [4],\n",
       "         [1]], device='cuda:0'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sentence_pairs[0])\n",
    "pair2tensors(fra_lang, eng_lang, sentence_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2],\n",
       "        [  3],\n",
       "        [ 50],\n",
       "        [792],\n",
       "        [  1]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"I'm a cat\"\n",
    "sentence2tensor(eng_lang,normalizeString(test))\n",
    "# sentence2indexlist(eng_lang,normalizeString(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, inout_dim, hidden_dim=256):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(inout_dim, hidden_dim)\n",
    "        self.encoder = nn.GRU(hidden_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        embedding = self.embedding(x).view(1,1,-1)\n",
    "        output, hidden = self.encoder(embedding,hidden)\n",
    "        return output,hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_dim, device=device)\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, inout_dim, hidden_dim=256):\n",
    "        super(DecoderRNN,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(inout_dim, hidden_dim)\n",
    "        self.encoder = nn.GRU(hidden_dim, hidden_dim)\n",
    "        self.hidden2out = nn.Linear(hidden_dim, inout_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim =1)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        embedding = self.embedding(x).view(1,1,-1)\n",
    "        output, hidden = self.encoder(embedding, hidden)\n",
    "        output = self.softmax(self.hidden2out(output[0]))\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_dim, device=device)\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = EncoderRNN(fra_lang.n_words).to(device)\n",
    "dec = DecoderRNN(eng_lang.n_words).to(device)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(itertools.chain(enc.parameters(),dec.parameters()),lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy pass into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['je suis employe de banque .', 'i m a bank clerk .']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['honest',\n",
       " 'carpenter',\n",
       " 'swamped',\n",
       " 'pressure',\n",
       " 'community',\n",
       " 'involved',\n",
       " 'considered']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_pair = random.choice(sentence_pairs)\n",
    "dummy_in = sentence2tensor(fra_lang, dummy_pair[0])\n",
    "dummy_out = sentence2tensor(eng_lang, dummy_pair[1])\n",
    "enc_h0 = enc.init_hidden()\n",
    "h = enc_h0\n",
    "\n",
    "print(dummy_pair)\n",
    "for i in range(dummy_in.size(0)):\n",
    "    _, h = enc(dummy_in[i],h)\n",
    "enc_h = h\n",
    "\n",
    "outs = []\n",
    "hidden = enc_h\n",
    "output = torch.tensor([[SOS_token]], device=device)\n",
    "for i in range(dummy_out.size(0)):\n",
    "    output, h = dec(output,hidden)\n",
    "\n",
    "    top_value, top_index = output.topk(1)  \n",
    "    output = top_index.squeeze().detach()\n",
    "    outs.append(output)\n",
    "    \n",
    "\n",
    "[eng_lang.index2word[o.item()] for o in outs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_example(input_tensor, target_tensor, enc, dec, optimizer, criterion, teacher_forcing=0.5):\n",
    "    encoder_hidden = enc.init_hidden()\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    target_length = input_tensor.size(0)\n",
    "    \n",
    "    for ei in range(target_length):\n",
    "        _, encoder_hidden = enc(input_tensor[ei], encoder_hidden)\n",
    "        \n",
    "    decoder_input = torch.tensor([[SOS_token]],device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    for di in range(target_tensor.size(0)):\n",
    "        decoder_input, decoder_hidden = dec(decoder_input, decoder_hidden)\n",
    "        # for nll output = [batchsize, number of classes (words)] and target = correct class\n",
    "        loss += criterion(decoder_input,target_tensor[di])\n",
    "        if random.random()<teacher_forcing:\n",
    "            decoder_input = target_tensor[di] #teacher forcing\n",
    "        else:\n",
    "            topv, topi = decoder_input.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "    return loss\n",
    "\n",
    "def train_step(input_tensor, target_tensor, enc, dec, optimizer, criterion):\n",
    "    loss = eval_example(input_tensor, target_tensor, enc, dec, optimizer, criterion, teacher_forcing=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()/ target_tensor.size(0)\n",
    "\n",
    "def test_evaluate(test_set):\n",
    "    \n",
    "    for i, pair in enumerate(test_set):\n",
    "        input_tensor = pair[0]\n",
    "        target_tensor = pair[1]\n",
    "        loss_total = 0\n",
    "        with torch.no_grad():\n",
    "            loss = eval_example(input_tensor, target_tensor, enc, dec, optimizer, criterion)\n",
    "            loss_total += loss.item()/ target_tensor.size(0)\n",
    "        return loss_total\n",
    "\n",
    "def train(training_set, test_set, epochs, print_every):\n",
    "    set_size = len(training_set)\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for i, pair in enumerate(training_set):\n",
    "            input_tensor = pair[0]\n",
    "            target_tensor = pair[1]\n",
    "            loss_total =0\n",
    "            loss = train_step(input_tensor, target_tensor, enc, dec, optimizer, criterion)\n",
    "            loss_total += loss\n",
    "\n",
    "            if ((e)*set_size + (i+1))%print_every ==0:\n",
    "                timestamp = f\"trainging time : {timedelta_string(datetime.now() - start_time)}\"\n",
    "                example_count = f\"examples {((e)*set_size + (i+1)):8}\"\n",
    "                eval_loss = test_evaluate(test_set)\n",
    "                print (f'epoch {e:3} | {example_count} | loss:{loss_total:5.2f} | test loss:{eval_loss:5.2f} | {timestamp}')\n",
    "                loss_total = 0\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total examples 14213\n"
     ]
    }
   ],
   "source": [
    "print(f\"total examples {len(sentence_pairs)}\")\n",
    "def train_test_split(total_examples, training_size = 1000,test_size = 100):\n",
    "    total_idx = [ i for i in range(len(total_examples))]\n",
    "    training_idx = [random.choice(total_idx) for _ in range(training_size)]\n",
    "    remaining_idx = list(set(total_idx) - set(training_idx))\n",
    "    test_idx = [random.choice(remaining_idx) for _ in range(test_size)]\n",
    "    assert set(test_idx).intersection(training_idx) == set([])\n",
    "    training_set = [ pair2tensors(fra_lang, eng_lang, total_examples[i]) for i in training_idx]\n",
    "    test_set = [pair2tensors(fra_lang, eng_lang, total_examples[i]) for i in test_idx]\n",
    "    return training_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, test_set = train_test_split(sentence_pairs, 13000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-401d7696d0f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-90-4cdd98bccc03>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(training_set, test_set, epochs, print_every)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mloss_total\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mloss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-9a76ab5a0f6d>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(input_tensor, target_tensor, enc, dec, optimizer, criterion)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(training_set, test_set, 10, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No attention: results\n",
    "\n",
    "    epoch   2 | examples    36000 | loss: 1.06 | test loss: 4.52 | trainging time : 0:00:09:44.56\n",
    "    \n",
    "With many of the random examples looking like:\n",
    "    \n",
    "    ['ils ne sont pas prepares a ca .', 'they re not prepared for this .']\n",
    "    ['they', 're', 'not', 'they', 're', 'not', 'they', 're']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.output_dim, self.hidden_dim)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "        self.attn = nn.Linear(self.hidden_dim*2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_dim * 2 , self.hidden_dim)\n",
    "        self.gru = nn.GRU(self.hidden_dim, self.hidden_dim)\n",
    "        self.out = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        embedded = self.dropout(self.embedding(x).view(1, 1, -1)) # (1, 1, h_dim)\n",
    "        \n",
    "        # (1,2 * h_dim) -> (1, max_l)\n",
    "        attn_weights = self.attn(torch.cat((hidden[0], embedded[0]), 1))\n",
    "        attn_weights = self.softmax(attn_weights) \n",
    "        \n",
    "        # (1,1,max_l) , (1,max_l,h_dim) - > (1, 1, h_dim)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        \n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1) # (1,1, 2 * h_dim)\n",
    "        output = self.relu(self.attn_combine(output).unsqueeze(0)) # (1,1, h_dim)\n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden, attn_weights\n",
    "        \n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_dim, device=device)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_dec = AttnDecoderRNN(hidden_dim=256, output_dim=eng_lang.n_words).to(device)\n",
    "attn_enc = EncoderRNN(fra_lang.n_words, hidden_dim=256).to(device)\n",
    "attn_optimizer = optim.SGD(itertools.chain(attn_enc.parameters(),attn_dec.parameters()),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_eval_example(input_tensor, target_tensor, attn_enc, attn_dec, attn_optimizer, criterion, teacher_forcing=0.5):\n",
    "    encoder_hidden = attn_enc.init_hidden()\n",
    "    attn_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    encoder_outputs = torch.zeros(MAX_LENGTH, attn_enc.hidden_dim, device=device)\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = attn_enc(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0,0]\n",
    "        \n",
    "    dec_input = torch.tensor([[SOS_token]],device=device)\n",
    "    dec_hidden = encoder_hidden\n",
    "    \n",
    "    \n",
    "    \n",
    "    for di in range(target_tensor.size(0)):\n",
    "        dec_input, dec_hidden, dec_attention = attn_dec(dec_input, dec_hidden, encoder_outputs)\n",
    "        # for nll output = [batchsize, number of classes (words)] and target = correct class\n",
    "        loss += criterion(dec_input,target_tensor[di])\n",
    "        \n",
    "        if random.random()<teacher_forcing:\n",
    "            dec_input = target_tensor[di] #teacher forcing\n",
    "        else:\n",
    "            topv, topi = dec_input.topk(1)\n",
    "            dec_input = topi.squeeze().detach()\n",
    "            if dec_input.item() == EOS_token:\n",
    "                break\n",
    "    return loss\n",
    "\n",
    "def attn_train_step(input_tensor, target_tensor, attn_enc, attn_dec, attn_optimizer, criterion):\n",
    "    loss = attn_eval_example(input_tensor, target_tensor, attn_enc, attn_dec, attn_optimizer, criterion, teacher_forcing=True)\n",
    "    loss.backward()\n",
    "    attn_optimizer.step()\n",
    "    return loss.item()/ target_tensor.size(0)\n",
    "\n",
    "def attn_test_evaluate(test_set):\n",
    "    \n",
    "    for i, pair in enumerate(test_set):\n",
    "        input_tensor = pair[0]\n",
    "        target_tensor = pair[1]\n",
    "        loss_total = 0\n",
    "        with torch.no_grad():\n",
    "            loss = attn_eval_example(input_tensor, target_tensor, attn_enc, attn_dec, attn_optimizer, criterion)\n",
    "            loss_total += loss.item()/ target_tensor.size(0)\n",
    "        return loss_total\n",
    "\n",
    "def attn_train(training_set, test_set, epochs, print_every):\n",
    "    set_size = len(training_set)\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for i, pair in enumerate(training_set):\n",
    "            input_tensor = pair[0]\n",
    "            target_tensor = pair[1]\n",
    "            loss_total =0\n",
    "            loss = attn_train_step(input_tensor, target_tensor, attn_enc, attn_dec, attn_optimizer, criterion)\n",
    "            loss_total += loss\n",
    "\n",
    "            if ((e)*set_size + (i+1))%print_every ==0:\n",
    "                timestamp = f\"trainging time : {timedelta_string(datetime.now() - start_time)}\"\n",
    "                example_count = f\"examples {((e)*set_size + (i+1)):8}\"\n",
    "                eval_loss = attn_test_evaluate(test_set)\n",
    "                print (f'epoch {e:3} | {example_count} | loss:{loss_total:5.2f} | test loss:{eval_loss:5.2f} | {timestamp}')\n",
    "                loss_total = 0\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_random_infer():\n",
    "    dummy_pair = random.choice(sentence_pairs)\n",
    "    dummy_in = sentence2tensor(fra_lang, dummy_pair[0])\n",
    "    dummy_out = sentence2tensor(eng_lang, dummy_pair[1])\n",
    "    enc_h0 = attn_enc.init_hidden()\n",
    "    h = enc_h0\n",
    "    \n",
    "    enc_outputs = torch.zeros(MAX_LENGTH, attn_enc.hidden_dim, device=device)\n",
    "    for i in range(dummy_in.size(0)):\n",
    "        o, h = attn_enc(dummy_in[i], h)\n",
    "        enc_outputs[i] = o[0,0]\n",
    "    enc_h = h\n",
    "\n",
    "    outs = []\n",
    "    hidden = enc_h\n",
    "    output = torch.tensor([[SOS_token]], device=device)\n",
    "    for i in range(dummy_out.size(0)*2):\n",
    "        output, hidden, attention_weights = attn_dec(output, hidden, enc_outputs)\n",
    "\n",
    "        top_value, top_index = output.topk(1)  \n",
    "        output = top_index.squeeze().detach()\n",
    "        outs.append(output)\n",
    "        if output.item()==EOS_token:\n",
    "            break\n",
    "\n",
    "    print(dummy_pair[0])\n",
    "    print(dummy_pair[1])\n",
    "    print(' '.join([eng_lang.index2word[o.item()] for o in outs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je suis toujours prudente .\n",
      "i m always careful .\n",
      "definitely superior stuck rugby rugby rugby faith nearsighted japanese mentally willed refugees\n"
     ]
    }
   ],
   "source": [
    "attn_random_infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-35-30662a3e9a48>(9)attn_eval_example()\n",
      "-> for ei in range(input_length):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  input_length\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  encoder_outputs.size()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 256])\n",
      "--KeyboardInterrupt--\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  q\n",
      "(Pdb)  q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-f069ef94ef2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mattn_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-30662a3e9a48>\u001b[0m in \u001b[0;36mattn_train\u001b[0;34m(training_set, test_set, epochs, print_every)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mloss_total\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_dec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mloss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-30662a3e9a48>\u001b[0m in \u001b[0;36mattn_train_step\u001b[0;34m(input_tensor, target_tensor, attn_enc, attn_dec, attn_optimizer, criterion)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mattn_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_dec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_eval_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_dec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mattn_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-30662a3e9a48>\u001b[0m in \u001b[0;36mattn_eval_example\u001b[0;34m(input_tensor, target_tensor, attn_enc, attn_dec, attn_optimizer, criterion, teacher_forcing)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mencoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_enc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mei\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-30662a3e9a48>\u001b[0m in \u001b[0;36mattn_eval_example\u001b[0;34m(input_tensor, target_tensor, attn_enc, attn_dec, attn_optimizer, criterion, teacher_forcing)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mencoder_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_enc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mei\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "attn_train(training_set, test_set, 10, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
